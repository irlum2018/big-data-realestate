{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p  /tmp/input\n",
    "! hadoop fs -put   -p  ./../data-clean/*.csv             /tmp/input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://92d0ac546444:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590763359188)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load Clean Dataset into a DataFrame from HDFS after wrangling is completed\n",
    "var df_clean = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/input/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))\n",
    "    .withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"Double\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"Double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change attributes into vectors \n",
    "\n",
    "#### Transform Sale Date into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Date\",unix_timestamp($\"Date\", \"dd/mm/yyyy\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set FeatureHasher for Suburb, StreetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{FeatureHasher, OneHotEncoder, StandardScaler, VectorAssembler}\n",
       "hasher: org.apache.spark.ml.feature.FeatureHasher = featureHasher_161fe125e6e7\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{FeatureHasher,OneHotEncoder,StandardScaler,VectorAssembler}\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OneHotEncoders for PropertyType,  MethodOfSale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ms_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_eaecb9672c30\n",
       "pt_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_658696ede5c3\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"MethodOfSale\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"PropertyType\")\n",
    "      .setOutputCol(\"pt_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble  the columns and column vectors into a single column - \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Array[String] = Array(Price, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec, m_sale_vec, pt_vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_c03bcb042059\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_71054d0534a7\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"ScaledFeatures\")\n",
    "      .setWithStd(true).setWithMean(true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into a Training and a Testing Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler,StandardScaler}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "     val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "    \n",
    "     (train, test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, MethodOfSale: int ... 11 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val (train, test) = train_test_split(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Predicted Price\")\n",
    "    .setMaxIter(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Stages to transform all columns into feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// add linear regression to stages\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//Construct the pipeline\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val lrPipe = new Pipeline().setStages(lrStages)\n",
    "\n",
    "//We fit our DataFrame into the pipeline to generate a model\n",
    "val lrModel = lrPipe.fit(train)\n",
    "\n",
    "//Make predictions using the model and the test data\n",
    "val predictions = lrModel.transform(test)\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "val rmse = new RegressionEvaluator()\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "  .setMetricName(\"rmse\")\n",
    "\n",
    "println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val r2 = new RegressionEvaluator()\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "  .setMetricName(\"r2\")\n",
    "\n",
    "println(\"R^2 on test data = \" + r2.evaluate(predictions))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-29 14:43:29,941 WARN  [CrossValidator-thread-pool-0] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-05-29 14:43:29,942 WARN  [CrossValidator-thread-pool-0] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.{ DataFrame, Row, SQLContext }\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")\n",
    "\n",
    "val lrPipeline = new Pipeline()\n",
    "    .setStages(lrStages)\n",
    "\n",
    "val cvLR = new CrossValidator()\n",
    "    .setEstimator(lrPipeline)\n",
    "    .setEvaluator(new RegressionEvaluator()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "    .setMetricName(\"rmse\"))\n",
    "    .setEstimatorParamMaps(lrParamMap)\n",
    "    .setNumFolds(5)\n",
    "    .setParallelism(2)\n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val cvLRModel = cvLR.fit(train)\n",
    "val lrPredictionsAndPrice = cvLRModel\n",
    "    .transform(test)\n",
    "    .select(\"Price\", \"Prediction\")\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictionsAndPrice.show()\n",
    "val bestLRModel = cvLRModel.bestModel\n",
    "    \n",
    "println(bestLRModel.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation\n",
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// utilit function to train and evaluate model\n",
    "// bug\n",
    "def train_eval[M <: PredictionModel[Vector, M]](\n",
    "    pipeline: Pipeline,\n",
    "    paramMap: Array[ParamMap],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "\n",
    "    val cv = new CrossValidator()\n",
    "      .setEstimator(pipeline)\n",
    "      .setEvaluator(new RegressionEvaluator()\n",
    "      .setLabelCol(\"Price\")\n",
    "      .setPredictionCol(\"Prediction\")\n",
    "      .setMetricName(\"rmse\"))\n",
    "      .setEstimatorParamMaps(paramMap)\n",
    "      .setNumFolds(5)\n",
    "      .setParallelism(2)\n",
    "\n",
    "    val cvModel = cv.fit(train)\n",
    "    val predictions = cvModel.transform(test)\n",
    "        \n",
    "    println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse.evaluate(predictions))\n",
    "    println(\"R^2 on test data = \" + r2.evaluate(predictions))\n",
    "\n",
    "    val bestModel = cvModel.bestModel\n",
    "    \n",
    "    println(bestModel.extractParamMap)\n",
    "    \n",
    "    bestModel\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")\n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "train_eval(new Pipeline().setStages(lrStages), lrParamMap, train, test)\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"MethodOfSale\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "df_featured = ms_encoder.transform(df_featured)\n",
    "df_featured = df_featured.drop(\"MethodOfSale\")\n",
    "\n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"PropertyType\")\n",
    "      .setOutputCol(\"pt_vec\")\n",
    "\n",
    "df_featured = pt_encoder.transform(df_featured)\n",
    "df_featured = df_featured.drop(\"PropertyType\")\n",
    "\n",
    "df_featured.select(\"str_name_suburb_vec\",\"m_sale_vec\",\"pt_vec\").show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Pipeline Estimator\n",
    "\n",
    "When you transform a column in your dataframe using pyspark.ml.feature.StringIndexer extra meta-data gets stored in the dataframe that specifically marks the transformed feature as a categorical feature.\n",
    "\n",
    "When you print the dataframe you will see a numeric value (which is an index that corresponds with one of your categorical values) and if you look at the schema you will see that your new transformed column is of type double. However, this new column you created with pyspark.ml.feature.StringIndexer.transform is not just a normal double column, it has extra meta-data associated with it that is very important. You can inspect this meta-data by looking at the metadata property of the appropriate field in your dataframe's schema (you can access the schema objects of your dataframe by looking at yourdataframe.schema)\n",
    "\n",
    "This extra metadata has two important implications:\n",
    "\n",
    "When you call .fit() when using a tree based model, it will scan the meta-data of your dataframe and recognize fields that you encoded as categorical with transformers such as pyspark.ml.feature.StringIndexer (as noted above there are other transformers that will also have this effect such as pyspark.ml.feature.VectorIndexer). Because of this, you DO NOT have to one-hot encode your features after you have transformed them with StringIndxer when using tree-based models in spark ML (however, you still have to perform one-hot encoding when using other models that do not naturally handle categoricals like linear regression, etc.).\n",
    "\n",
    "Because this metadata is stored in the data frame, you can use pyspark.ml.feature.IndexToString to reverse the numeric indices back to the original categorical values (which are often strings) at any time.\n",
    "\n",
    "**Build Random Forest model**\n",
    "\n",
    "Specify maxDepth, maxBins, impurity, auto and seed parameters.\n",
    "maxDepth: Maximum depth of a tree. Increasing the depth makes the model more powerful, but deep trees take longer to train.\n",
    "\n",
    "maxBins: Maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node.\n",
    "\n",
    "impurity:Criterion used for information gain calculation\n",
    "\n",
    "auto:Automatically select the number of features to consider for splits at each tree node\n",
    "\n",
    "seed:Use a random seed number , allowing to repeat the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "// Models hypoparameters\n",
    "//val algorithm = Algo.Classification\n",
    "//val impurity = Gini\n",
    "//val maximumDepth = 3\n",
    "//val treeCount = 20\n",
    "//val featureSubsetStrategy = \"auto\"\n",
    "val seed = 5043\n",
    "\n",
    "estimator = RandomForestRegressor()\n",
    "evaluator = RegressionEvaluator()\n",
    "paramGrid = ParamGridBuilder().addGrid(estimator.numTrees, [2,3])\\\n",
    "                              .addGrid(estimator.maxDepth, [2,3])\\\n",
    "                              .addGrid(estimator.impurity, ['variance'])\\\n",
    "                              .addGrid(estimator.featureSubsetStrategy, ['sqrt'])\\\n",
    "                              .build()\n",
    "pipeline = Pipeline(stages=[estimator])\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "cvModel = crossval.fit(result)\n",
    "\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setMaxBins(4)\n",
    "  .setMaxDepth(2)\n",
    "  .setNumTrees(10)\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "  .setSeed(seed)\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "val rfStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            rf\n",
    ")\n",
    "\n",
    "val rfPipe = new Pipeline().setStages(rfStages) \n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "val rfModel = rfPipe.fit(train)\n",
    "\n",
    "//Make predictions using the model and the test data\n",
    "val rfPredictions = rfModel.transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{p.name: v for p, v in m.items()} for m in cvModel.getEstimatorParamMaps()]\n",
    "Th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cv = new CrossValidator().setNumFolds(10).setEstimator(pipeline).\n",
    "             setEvaluator(new BinaryClassificationEvaluator)\n",
    "val cmModel = cv.fit(train)\n",
    "\n",
    "val rfCVPredictions = cmModel.transform(test)\n",
    "\n",
    "rfCVPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test/Validation split: 60%/20%20%\n",
    "\n",
    "shaffle before splitting\n",
    "train model on train , tune on validation, test on test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n",
    "\n",
    "\n",
    "Mean squared error (MSE) is defined as the average of squared differences between the predicted outcome and the true outcome. \n",
    "\n",
    "R2 coefficient represents the proportion of variance in the outcome that our model is capable of predicting based on its features.\n",
    "\n",
    "**Bias vs Variance**\n",
    "Graph of Error (validation error and training error) versus training set size. They should converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Apache Spark (n.d.). _Spark ML Programming Guide._ Retrieved from https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "\n",
    "Gorczynski M. (2017). _Introduction to machine learning with spark and mllib (dataframe API)._ Retrieved from https://scalac.io/scala-spark-ml-machine-learning-introduction/\n",
    "\n",
    "Hydrospheredata (2020). _Program creek. Scala Code Examples. Scaler_ Retrieved from https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "Johnson S (2019). _From sckit-learn to Spark ML._ Retrieved from \n",
    "https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852\n",
    "\n",
    "\n",
    "Johnson S (2019). _Housing Prices - Spark ML Project_ Retrieved from https://github.com/scottdjohnson/HousingPricePredictions/blob/master/HousingPrices-SparkML.ipynb\n",
    "\n",
    "Masri A. (2019). _FeatureTransformation._ Retrieved from \n",
    "\n",
    "https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n",
    "\n",
    "\n",
    "Scala Doc (n.d.)  Retrieved from https://docs.scala-lang.org\n",
    "\n",
    "Jen G. (2020) _FeatureHasher._ Retrieved from https://george-jen.gitbook.io/data-science-and-apache-spark/featurehasher\n",
    "\n",
    "\n",
    "(2019) _Random Forest Classifier with Apache Spark_ Retireved from https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.FeatureHasher\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")\n",
    "\n",
    "var df_featured = hasher.transform(df_clean)\n",
    "\n",
    "df_featured = df_featured.drop(\"StreetName\").drop(\"Suburb\")\n",
    "df_featured.select(\"str_name_suburb_vec\").show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
