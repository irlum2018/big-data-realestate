{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/input/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p  /tmp/input\n",
    "! hadoop fs -put   -p  ./../data-clean/*.csv             /tmp/input         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check Spark Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://6228915d44c3:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590929770377)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.driver.port,42725)\n",
      "(spark.repl.class.uri,spark://6228915d44c3:42725/classes)\n",
      "(spark.repl.class.outputDir,/tmp/tmp7t3xap36)\n",
      "(spark.executor.id,driver)\n",
      "(spark.app.id,local-1590929770377)\n",
      "(spark.app.name,spylon-kernel)\n",
      "(spark.executor.memory,6g)\n",
      "(spark.driver.memory,6g)\n",
      "(spark.rdd.compress,True)\n",
      "(spark.driver.host,6228915d44c3)\n",
      "(spark.serializer.objectStreamReset,100)\n",
      "(spark.master,local[*])\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.ui.showConsoleProgress,true)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.{SparkConf, SparkContext}\n",
       "cs: org.apache.spark.SparkConf = org.apache.spark.SparkConf@76a716c0\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.{SparkConf,SparkContext}\n",
    "\n",
    "val cs = spark.sparkContext.getConf\n",
    "sc.getConf.getAll.foreach { println }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n",
       "res1: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load Clean Dataset into a DataFrame from HDFS after wrangling is completed\n",
    "var df_clean = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/input/*.csv\")\n",
    "df_clean.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))\n",
    "    .withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"Double\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"Double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.cache()\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct vectors from attributes\n",
    "#### Transform Sale Date into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Date\",unix_timestamp($\"Date\", \"dd/mm/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set FeatureHasher for Suburb, StreetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{FeatureHasher, OneHotEncoder, StandardScaler, VectorAssembler}\n",
       "hasher: org.apache.spark.ml.feature.FeatureHasher = featureHasher_f692100607ed\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{FeatureHasher,OneHotEncoder,StandardScaler,VectorAssembler}\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OneHotEncoders for PropertyType, MethodOfSale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ms_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_2a66214ad440\n",
       "pt_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_c35c6735200d\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"MethodOfSale\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"PropertyType\")\n",
    "      .setOutputCol(\"pt_vec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the columns and column vectors into a single column - \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Array[String] = Array(DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec, m_sale_vec, pt_vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_2ecf1e388057\n",
       "dd: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Array(\"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "val dd = hasher.transform(df_clean).drop(\"StreetName\",\"Suburb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mm: org.apache.spark.sql.DataFrame = [Price: double, PropertyType: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mm = ms_encoder.transform(dd).drop(\"MethodOfSale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pt: org.apache.spark.sql.DataFrame = [Price: double, DistanceFromCBD: double ... 10 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pt = pt_encoder.transform(mm).drop(\"PropertyType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_ds: org.apache.spark.sql.DataFrame = [Price: double, features: vector]\n",
       "res4: feature_ds.type = [Price: double, features: vector]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val feature_ds = assembler.transform(pt).drop(\"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "feature_ds.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    Price|            features|\n",
      "+---------+--------------------+\n",
      "|1480000.0|(262163,[0,1,2,3,...|\n",
      "|1035000.0|(262163,[0,1,2,4,...|\n",
      "|1465000.0|(262163,[0,1,2,4,...|\n",
      "| 850000.0|(262163,[0,1,2,3,...|\n",
      "|1600000.0|(262163,[0,1,2,3,...|\n",
      "| 941000.0|(262163,[0,1,2,4,...|\n",
      "|1876000.0|(262163,[0,1,2,4,...|\n",
      "|1636000.0|(262163,[0,1,2,3,...|\n",
      "|1097000.0|(262163,[0,1,2,3,...|\n",
      "|1350000.0|(262163,[0,1,2,3,...|\n",
      "|1172500.0|(262163,[0,1,2,3,...|\n",
      "|1310000.0|(262163,[0,1,2,3,...|\n",
      "|1200000.0|(262163,[0,1,2,3,...|\n",
      "|1176500.0|(262163,[0,1,2,3,...|\n",
      "| 955000.0|(262163,[0,1,2,4,...|\n",
      "| 890000.0|(262163,[0,1,2,3,...|\n",
      "|1330000.0|(262163,[0,1,2,3,...|\n",
      "|1090000.0|(262163,[0,1,2,3,...|\n",
      "|1100000.0|(262163,[0,1,2,3,...|\n",
      "|1315000.0|(262163,[0,1,2,4,...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_95f031774569\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"scaledFeatures\")\n",
    "      .setWithStd(true).setWithMean(true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into a Training and a Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "     val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "    \n",
    "     (train, test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, features: vector]\n",
       "res6: test.type = [Price: double, features: vector]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val (train, test) = train_test_split(feature_ds)\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_f405be71f6ac\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setMaxIter(1500)\n",
    "    .setRegParam(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res7: String =\n",
       "aggregationDepth: suggested depth for treeAggregate (>= 2) (default: 2)\n",
       "elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty (default: 0.0)\n",
       "epsilon: The shape parameter to control the amount of robustness. Must be > 1.0. (default: 1.35)\n",
       "featuresCol: features column name (default: features)\n",
       "fitIntercept: whether to fit an intercept term (default: true)\n",
       "labelCol: label column name (default: label, current: Price)\n",
       "loss: The loss function to be optimized. Supported options: squaredError, huber. (Default squaredError) (default: squaredError)\n",
       "maxIter: maximum number of iterations (>= 0) (default: 100, current: 1500)\n",
       "predictionCol: prediction column name (default: prediction)\n",
       "regParam: ..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.explainParams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time: [R](block: => R)R\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time[R](block: => R): R = {\n",
    "  val t0 = System.nanoTime()\n",
    "  val result = block    // call-by-name\n",
    "  val t1 = System.nanoTime()\n",
    "  println(\"Elapsed time: \" + (t1 - t0)/1000000000 + \" s\")\n",
    "  result\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Predictor\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.PredictionModel\n",
       "import org.apache.spark.ml.Pipeline\n",
       "predictions: [R <: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], M <: org.apache.spark.ml.PredictionModel[org.apache.spark.ml.linalg.Vector,M]](predictor: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "def predictions[R <: Predictor[Vector, R, M],\n",
    "                M <: PredictionModel[Vector, M]](\n",
    "    predictor: Predictor[Vector, R, M],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "    \n",
    "    val pipeline = new Pipeline()\n",
    "      .setStages(Array(scaler, predictor))\n",
    "     \n",
    "    val result =pipeline.fit(train).transform(test)\n",
    "    result\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 12:57:14,304 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-05-31 12:57:14,305 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "Elapsed time: 239 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrPredictions: org.apache.spark.sql.DataFrame = [Price: double, features: vector ... 2 more fields]\n",
       "res8: lrPredictions.type = [Price: double, features: vector ... 2 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPredictions = time{predictions(lr, train, test)}\n",
    "lrPredictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res9: Array[String] = Array(Price, features, scaledFeatures, prediction)\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 13:01:14,937 WARN  [Executor task launch worker for task 162] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_265_0 in memory! (computed 2.5 GB so far)\n",
      "2020-05-31 13:01:14,942 WARN  [Executor task launch worker for task 162] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_265_0 to disk instead.\n",
      "2020-05-31 13:01:27,358 WARN  [Executor task launch worker for task 162] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_265_0 in memory! (computed 2.5 GB so far)\n",
      "+--------+----------+\n",
      "|   Price|prediction|\n",
      "+--------+----------+\n",
      "|170000.0|  359905.0|\n",
      "|280000.0| -121507.0|\n",
      "|280500.0|  411023.0|\n",
      "|283000.0| -244894.0|\n",
      "|290000.0|  671624.0|\n",
      "|300000.0|  576442.0|\n",
      "|300000.0|  120603.0|\n",
      "|305000.0|  341858.0|\n",
      "|310000.0|   -6485.0|\n",
      "|316000.0|  -33210.0|\n",
      "|320000.0|  311037.0|\n",
      "|320000.0|  602258.0|\n",
      "|320000.0|  468252.0|\n",
      "|320000.0|   99955.0|\n",
      "|325000.0|  295665.0|\n",
      "|333000.0| -229038.0|\n",
      "|340000.0| -324449.0|\n",
      "|345000.0|  817134.0|\n",
      "|348000.0| -249289.0|\n",
      "|350000.0| 3421399.0|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrPredictions.withColumn(\"prediction\", round($\"prediction\", 0)).select(\"Price\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "evaluate: (predictions: org.apache.spark.sql.DataFrame, metric: String)Unit\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "def evaluate ( predictions: DataFrame, metric: String) = {\n",
    "    val eval =  new RegressionEvaluator()\n",
    "       .setLabelCol(\"Price\")\n",
    "       .setPredictionCol(\"prediction\")\n",
    "       .setMetricName(metric)\n",
    "println(\"Root Mean Squared Error \"+  metric.toUpperCase()+\" on test data = \" + eval.evaluate(predictions))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n",
    "\n",
    "**Mean squared error (MSE)** -- the average of squared differences between the predicted outcome and the true outcome.\n",
    "\n",
    "**R2 coefficient** -- the proportion of variance in the outcome that our model is capable of predicting based on its features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 13:01:46,817 WARN  [Executor task launch worker for task 163] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_265_0 in memory! (computed 2.5 GB so far)\n",
      "Root Mean Squared Error RMSE on test data = 429164.39732878417\n"
     ]
    }
   ],
   "source": [
    "evaluate(lrPredictions,\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 13:01:53,101 WARN  [Executor task launch worker for task 164] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_265_0 in memory! (computed 2.5 GB so far)\n",
      "Root Mean Squared Error R2 on test data = 0.5571150795620388\n"
     ]
    }
   ],
   "source": [
    "evaluate(lrPredictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "<span style=\"color:red\">\n",
    "TO DO: does not finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "\n",
    "\n",
    "def train_eval[R <: Predictor[Vector, R, M],\n",
    "               M <: PredictionModel[Vector, M]](\n",
    "    predictor: Predictor[Vector, R, M],\n",
    "    paramMap: Array[ParamMap],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "\n",
    "    val pipeline = new Pipeline()\n",
    "      .setStages(Array(scaler, predictor))\n",
    "    \n",
    "    val cv = new CrossValidator()\n",
    "        .setEstimator(pipeline)\n",
    "        .setEvaluator(new RegressionEvaluator()\n",
    "        .setLabelCol(\"Price\")\n",
    "        .setPredictionCol(\"prediction\")\n",
    "        .setMetricName(\"rmse\"))\n",
    "        .setEstimatorParamMaps(paramMap)\n",
    "        .setNumFolds(5)\n",
    "        .setParallelism(2)\n",
    "\n",
    "    val cvModel = cv.fit(train)\n",
    "    val predictions = cvModel.transform(test)\n",
    "    \n",
    "    predictions.cache()\n",
    "    evaluate(predictions,\"rmse\")\n",
    "    evaluate(predictions,\"r2\")\n",
    "    \n",
    "    val bestModel = cvModel.bestModel\n",
    "    \n",
    "    println(bestModel.extractParamMap)\n",
    "    \n",
    "    bestModel\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"prediction\")\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10,1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0,0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10, 50, 100, 500, 800))\n",
    "    .build()\n",
    "\n",
    "val t0 = System.nanoTime()\n",
    "val bestLRModel = train_eval(lr, lrParamMap, train, test)\n",
    "val t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0)/(1000000000) + \" s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest Regression\n",
    "\n",
    "**Build Random Forest model**\n",
    "Specify maxDepth, maxBins, auto and seed parameters.\n",
    "\n",
    "**maxDepth** -- Maximum depth of a tree. Increasing the depth makes the model more powerful, but deep trees take longer to train.\n",
    "\n",
    "**maxBins** -- Maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node.\n",
    "\n",
    "**auto** -- Automatically select the number of features to consider for splits at each tree node\n",
    "\n",
    "**seed** -- Use a random seed number , allowing to repeat the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the number of trees is 1, then no bootstrapping is used at all. However, if the number of trees is > 1, then the bootstrapping is accomplished. Where, the parameter featureSubsetStrategy signifies the number of features to be considered for splits at each node. The supported values of featureSubsetStrategy are \"auto\", \"all\", \"sqrt\", \"log2\" and \"on third\". The supported numerical values, on the other hand, are (0.0-1.0] and [1-n]. However, if featureSubsetStrategy is chosen as \"auto\", the algorithm chooses the best feature subset strategy automatically\n",
    "\n",
    "\n",
    "If the numTrees == 1, the featureSubsetStrategy is set to be \"all\". However, if the numTrees > 1 (i.e., forest), featureSubsetStrategy is set to be \"onethird\" for regression\n",
    "\n",
    "\n",
    "Moreover, if a real value \"n\" is in the range (0, 1.0] is set, n*number_of_features is used consequently. However, if an integer value \"n\" is in the range (1, the number of features) is set, only n features are used alternatively\n",
    "\n",
    "\n",
    "The parameter categoricalFeaturesInfo which is a map is used for storing arbitrary of categorical features. An entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1,...,k-1}\n",
    "The impurity criterion used for information gain calculation. The supported values are “gini\" and “variance”. The former is the only supported value for classification. The latter is used for regression\n",
    "\n",
    "\n",
    "The maxDepth is the maximum depth of the tree. (e.g., depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes). However, the suggested value is 4 to get a better result\n",
    "\n",
    "\n",
    "The maxBins signifies the maximum number of bins used for splitting the features; where the suggested value is 100 to get better results\n",
    "\n",
    "\n",
    "Finally, the random seed is used for bootstrapping and choosing feature subsets to avoid the random nature of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.RandomForestRegressor\n",
       "import org.apache.spark.ml.tuning.CrossValidator\n",
       "seed: Int = 5043\n",
       "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_361d524d0dd4\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "//import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val seed = 5043\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setMaxBins(100)\n",
    "  .setMaxDepth(4)\n",
    "  .setNumTrees(10)\n",
    "  .setFeatureSubsetStrategy(\"onethird\")\n",
    "  .setSeed(seed)\n",
    "  .setLabelCol(\"Price\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 13:08:52,128 WARN  [Executor task launch worker for task 170] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_319_0 in memory! (computed 2.1 GB so far)\n",
      "2020-05-31 13:08:52,128 WARN  [Executor task launch worker for task 170] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_319_0 to disk instead.\n",
      "2020-05-31 13:09:26,962 WARN  [Executor task launch worker for task 170] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_319_0 in memory! (computed 2.1 GB so far)\n",
      "2020-05-31 13:10:58,041 WARN  [Executor task launch worker for task 172] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_319_0 in memory! (computed 2.1 GB so far)\n"
     ]
    }
   ],
   "source": [
    "val rfPredictions = time{predictions(rf, train, test)}\n",
    "rfPredictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"prediction\", round($\"prediction\", 0)).select(\"Price\",\"prediction\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rfPredictions,\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rfPredictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "<span style=\"color:red\">\n",
    "TO DO: \n",
    "* finish implementation for Cross-validation \n",
    "* check if finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "\n",
    "// Models hypoparameters\n",
    "val numTrees = Seq(5)//,10,15)\n",
    "val maxBins = Seq(2)//,5,10)\n",
    "val maxDepth = Seq(2)//,3,5)\n",
    "//val impurity = Seq(\"gini\")//,\"entropy\",\"variance\",)\n",
    "val featureSubsetStrategy = Seq(\"sqrt\")\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"prediction\")\n",
    "\n",
    "\n",
    "val rfParamMap = new ParamGridBuilder()\n",
    "  .addGrid(rf.numTrees, numTrees)\n",
    "  .addGrid(rf.maxDepth, maxDepth)\n",
    "  .addGrid(rf.maxBins, maxBins)\n",
    "  .addGrid(rf.featureSubsetStrategy, featureSubsetStrategy)\n",
    "  .build()\n",
    "\n",
    "val t0 = System.nanoTime()\n",
    "val best_model = train_eval(rf, lrParamMap, train, test)\n",
    "val t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0)/(1000000000) + \" s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"prediction\", round($\"prediction\", 0)).select(\"Price\",\"prediction\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias vs Variance Graph of Error (validation error and training error) versus training set size. \n",
    "\n",
    "\n",
    "<span style=\"color:red\">\n",
    "TO DO: \n",
    "produce graph -- validation error and training error should converge\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Apache Spark (n.d.). Spark ML Programming Guide. Retrieved from https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "\n",
    "Gorczynski M. (2017). Introduction to machine learning with spark and mllib (dataframe API). Retrieved from https://scalac.io/scala-spark-ml-machine-learning-introduction/\n",
    "\n",
    "Hydrospheredata (2020). Program creek. Scala Code Examples. Scaler Retrieved from https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "Jen G. (2020) FeatureHasher. Retrieved from https://george-jen.gitbook.io/data-science-and-apache-spark/featurehasher\n",
    "\n",
    "Johnson S (2019). From sckit-learn to Spark ML. Retrieved from https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852\n",
    "\n",
    "Johnson S (2019). Housing Prices - Spark ML Project Retrieved from https://github.com/scottdjohnson/HousingPricePredictions/blob/master/HousingPrices-SparkML.ipynb\n",
    "\n",
    "Masri A. (2019). FeatureTransformation. Retrieved from\n",
    "https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n",
    "\n",
    "Scala Doc (n.d.) Retrieved from https://docs.scala-lang.org\n",
    "\n",
    "\n",
    "(2019) Random Forest Classifier with Apache Spark Retireved from https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
