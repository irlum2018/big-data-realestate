{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `.': No such file or directory\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p  /tmp/output\n",
    "! hadoop fs -put   -p  /../data-clean/*.csv             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load Clean Dataset into a DataFrame from HDFS after wrangling is completed\n",
    "var df_clean = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/output/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))\n",
    "    .withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"Double\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"Double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into a Training and a Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.DataFrame, org.apache.spark.sql.DataFrame)\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "    val assembler = new VectorAssembler().\n",
    "       setInputCols(data.drop(\"Price\").columns).\n",
    "       setOutputCol(\"features\")\n",
    "    \n",
    "    val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "\n",
    "    (assembler.transform(train), assembler.transform(test))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " Data type string of column Suburb is not supported.",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalArgumentException: Data type string of column Suburb is not supported.",
      "Data type string of column Date is not supported.",
      "Data type string of column StreetName is not supported.",
      "  at org.apache.spark.ml.feature.VectorAssembler.transformSchema(VectorAssembler.scala:169)",
      "  at org.apache.spark.ml.PipelineStage.transformSchema(Pipeline.scala:74)",
      "  at org.apache.spark.ml.feature.VectorAssembler.transform(VectorAssembler.scala:86)",
      "  at train_test_split(<console>:52)",
      "  ... 44 elided",
      ""
     ]
    }
   ],
   "source": [
    "\n",
    "val (train, test) = train_test_split(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale and Center the data.\n",
    "\n",
    "Our data is grouped into:\n",
    "\n",
    "categorical:\n",
    "\n",
    "* nominal: MethodOfSale, PropertyType, Suburb, StreetName\n",
    "* ordinal: Date\n",
    "\n",
    "numerical: Price, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming categorical nominal values for Type and Method of Sale to features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "case class PropertyType(id: Integer, p_type: String)\n",
    "\n",
    "val propertyTypeDS = Seq(PropertyType(1, \"H\"), PropertyType(2, \"U\"),PropertyType(3, \"T\")).toDS()\n",
    "\n",
    "case class MethodOfSale(id: Integer, m_sale: String)\n",
    "\n",
    "val methodOfSaleDS = Seq(MethodOfSale(1, \"S\"), MethodOfSale(2, \"SP\"),MethodOfSale(3, \"PI\"),\n",
    "                       MethodOfSale(4, \"PN\"), MethodOfSale(5, \"SN\"),MethodOfSale(6, \"VB\"),\n",
    "                       MethodOfSale(7, \"W\"), MethodOfSale(8, \"SA\"),MethodOfSale(9, \"SS\")).toDS()\n",
    "\n",
    "methodOfSaleDS.show()\n",
    "propertyTypeDS.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.{CountVectorizer, OneHotEncoder, StringIndexer}\n",
    "import org.apache.spark.sql.SparkSession\n",
    "\n",
    "//transforming PropertyType\n",
    "val pt_indexer = new StringIndexer().setInputCol(\"p_type\")\n",
    "      .setOutputCol(\"pt_indexed\")\n",
    "      .fit(propertyTypeDS)\n",
    "val pt_indexed = pt_indexer.transform(propertyTypeDS)\n",
    "    \n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"pt_indexed\")\n",
    "      .setOutputCol(\"pt_vec\")\n",
    "\n",
    "val pt_encoded = pt_encoder.transform(pt_indexed)\n",
    "    pt_encoded.select(\"id\", \"pt_vec\").show()\n",
    "\n",
    "\n",
    "//transforming MethodOfSale\n",
    "val ms_indexer = new StringIndexer().setInputCol(\"m_sale\")\n",
    "      .setOutputCol(\"m_sale_indexed\")\n",
    "      .fit(methodOfSaleDS)\n",
    "val ms_indexed = ms_indexer.transform(methodOfSaleDS)\n",
    "\n",
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"m_sale_indexed\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "val ms_encoded = ms_encoder.transform(ms_indexed)\n",
    "    ms_encoded.select(\"id\", \"m_sale_vec\").show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scaling numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "// StandardScaler https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator-implements a method fit(), which accepts a DataFrame and produces a Model-Transformer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "<console>",
     "evalue": "64: error: not found: type RegressionEvaluator",
     "output_type": "error",
     "traceback": [
      "<console>:64: error: not found: type RegressionEvaluator",
      "             .setEvaluator(new RegressionEvaluator()",
      "                               ^",
      "<console>:75: error: not found: value rmse",
      "           println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse.evaluate(predictions))",
      "                                                                      ^",
      "<console>:76: error: not found: value r2",
      "           println(\"R^2 on test data = \" + r2.evaluate(predictions))",
      "                                           ^",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "// utilit function to train and evaluate model\n",
    "// bug\n",
    "def train_eval[R <: Predictor[Vector, R, M],\n",
    "               M <: PredictionModel[Vector, M]](\n",
    "    predictor: Predictor[Vector, R, M],\n",
    "    paramMap: Array[ParamMap],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "\n",
    "    val cv = new CrossValidator()\n",
    "      .setEstimator( predictor    \n",
    "                    .setLabelCol(\"lastsoldprice\")\n",
    "                    .setFeaturesCol(\"features\"))\n",
    "      .setEvaluator(new RegressionEvaluator()\n",
    "          .setLabelCol(\"lastsoldprice\")\n",
    "          .setPredictionCol(\"prediction\")\n",
    "          .setMetricName(\"rmse\"))\n",
    "      .setEstimatorParamMaps(paramMap)\n",
    "      .setNumFolds(5)\n",
    "      .setParallelism(2)\n",
    "\n",
    "    val cvModel = cv.fit(train)\n",
    "    val predictions = cvModel.transform(test)\n",
    "    \n",
    "    println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse.evaluate(predictions))\n",
    "    println(\"R^2 on test data = \" + r2.evaluate(predictions))\n",
    "\n",
    "    val bestModel = cvModel.bestModel\n",
    "    \n",
    "    println(bestModel.extractParamMap)\n",
    "    \n",
    "    bestModel\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Apache Spark (n.d.). _Spark ML Programming Guide._ Retrieved from https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "\n",
    "Hydrospheredata (n.d.). _org.apache.spark.ml.feature.StandardScaler Scala Examples._ Retrieved from https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852\n",
    "\n",
    "Johnson S (2019). _From sckit-learn to Spark ML._ Retrieved from https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n",
    "Masri A. (2019). _FeatureTransformation._ Retrieved from https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
