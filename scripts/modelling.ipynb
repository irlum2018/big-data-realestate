{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/input/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p  /tmp/input\n",
    "! hadoop fs -put   -p  ./../data-clean/*.csv             /tmp/input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Clean Dataset into a DataFrame from HDFS \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load Clean Dataset into a DataFrame from HDFS after wrangling is completed\n",
    "var df_clean = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/input/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))\n",
    "    .withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"Double\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"Double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change attributes into vectors \n",
    "\n",
    "#### Transform Sale Date into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Date\",unix_timestamp($\"Date\", \"dd/mm/yyyy\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set FeatureHasher for Suburb, StreetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{FeatureHasher, OneHotEncoder, StandardScaler, VectorAssembler}\n",
       "hasher: org.apache.spark.ml.feature.FeatureHasher = featureHasher_c52632897fac\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{FeatureHasher,OneHotEncoder,StandardScaler,VectorAssembler}\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OneHotEncoders for PropertyType,  MethodOfSale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ms_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_84595a7a6062\n",
       "pt_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_c0f4033f6254\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"MethodOfSale\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"PropertyType\")\n",
    "      .setOutputCol(\"pt_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble  the columns and column vectors into a single column - \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Array[String] = Array(Price, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec, m_sale_vec, pt_vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_ec554a896ced\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_ecc6c9d022aa\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"ScaledFeatures\")\n",
    "      .setWithStd(true).setWithMean(true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into a Training and a Testing Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{VectorAssembler,StandardScaler}\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "     val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "    \n",
    "     (train, test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, MethodOfSale: int ... 11 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val (train, test) = train_test_split(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.Pipeline\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_bd364196d01b\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Predicted Price\")\n",
    "    .setMaxIter(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Stages to transform all columns into feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrStages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(featureHasher_c52632897fac, oneHot_84595a7a6062, oneHot_c0f4033f6254, vecAssembler_ec554a896ced, linReg_bd364196d01b)\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// add linear regression to stages\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-29 22:37:16,724 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-05-29 22:37:16,725 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "pipeline was executed 131"
     ]
    },
    {
     "data": {
      "text/plain": [
       "startTimeMillis: Long = 1590791827522\n",
       "lrPipe: org.apache.spark.ml.Pipeline = pipeline_b442e1ff4d19\n",
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_b442e1ff4d19\n",
       "predictions: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 16 more fields]\n",
       "endTimeMillis: Long = 1590791958860\n",
       "durationSeconds: Long = 131\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Construct the pipeline\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val lrPipe = new Pipeline().setStages(lrStages)\n",
    "\n",
    "//We fit our DataFrame into the pipeline to generate a model\n",
    "val lrModel = lrPipe.fit(train)\n",
    "\n",
    "//Make predictions using the model and the test data\n",
    "val predictions = lrModel.transform(test)\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res11: Array[String] = Array(Price, MethodOfSale, PropertyType, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Suburb, Date, StreetName, str_name_suburb_vec, m_sale_vec, pt_vec, features, Predicted Price)\n"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res12: Long = 3108\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res13: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 16 more fields]\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|   Price|Predicted Price|\n",
      "+--------+---------------+\n",
      "|170000.0|       170064.0|\n",
      "|280000.0|       280013.0|\n",
      "|280500.0|       280579.0|\n",
      "|283000.0|       283663.0|\n",
      "|290000.0|       290036.0|\n",
      "|300000.0|       300055.0|\n",
      "|300000.0|       299881.0|\n",
      "|305000.0|       305248.0|\n",
      "|310000.0|       310135.0|\n",
      "|316000.0|       315935.0|\n",
      "|320000.0|       320027.0|\n",
      "|320000.0|       320035.0|\n",
      "|320000.0|       320560.0|\n",
      "|320000.0|       319303.0|\n",
      "|325000.0|       325090.0|\n",
      "|333000.0|       332989.0|\n",
      "|340000.0|       342929.0|\n",
      "|345000.0|       345065.0|\n",
      "|348000.0|       348240.0|\n",
      "|350000.0|       350108.0|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "evaluate: (predictions: org.apache.spark.sql.DataFrame, metric: String)Unit\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate ( predictions: DataFrame, metric: String) = {\n",
    "    val eval =  new RegressionEvaluator()\n",
    "       .setLabelCol(\"Price\")\n",
    "       .setPredictionCol(\"Predicted Price\")\n",
    "       .setMetricName(metric)\n",
    "\n",
    "println(\"Root Mean Squared Error \"+  metric.toUpperCase()+\" on test data = \" + eval.evaluate(predictions))\n",
    "\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error RMSE on test data = 975.1264181243374\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions,\"rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error R2 on test data = 0.9999977135307482\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "<span style='color:red'> does not finish run in reasonable time</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{ DataFrame, Row, SQLContext }\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")\n",
    "\n",
    "val lrPipeline = new Pipeline()\n",
    "    .setStages(lrStages)\n",
    "\n",
    "val cvLR = new CrossValidator()\n",
    "    .setEstimator(lrPipeline)\n",
    "    .setEvaluator(new RegressionEvaluator()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "    .setMetricName(\"rmse\"))\n",
    "    .setEstimatorParamMaps(lrParamMap)\n",
    "    .setNumFolds(5)\n",
    "    .setParallelism(2)\n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val cvLRModel = cvLR.fit(train)\n",
    "val lrPredictionsAndPrice = cvLRModel\n",
    "    .transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictionsAndPrice.show()\n",
    "val bestLRModel = cvLRModel.bestModel\n",
    "    \n",
    "println(bestLRModel.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictionsAndPrice.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Random Forest model\n",
    "\n",
    "Specify maxDepth, maxBins, impurity, auto and seed parameters.\n",
    "\n",
    "**maxDepth** -- Maximum depth of a tree. Increasing the depth makes the model more powerful, but deep trees take longer to train.\n",
    "\n",
    "**imaxBins** -- Maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node.\n",
    "\n",
    "**impurity** -- Criterion used for information gain calculation\n",
    "\n",
    "**iauto** -- Automatically select the number of features to consider for splits at each tree node\n",
    "\n",
    "**iseed** -- Use a random seed number , allowing to repeat the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-29 22:17:18,788 WARN  [Executor task launch worker for task 7] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_42_0 in memory! (computed 272.9 MB so far)\n",
      "2020-05-29 22:17:18,794 WARN  [Executor task launch worker for task 7] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_42_0 to disk instead.\n",
      "2020-05-29 22:17:59,540 WARN  [Executor task launch worker for task 7] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_42_0 in memory! (computed 272.9 MB so far)\n",
      "2020-05-29 22:19:28,512 WARN  [Executor task launch worker for task 9] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_42_0 in memory! (computed 272.9 MB so far)\n",
      "pipeline was executed 386"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.RandomForestRegressor\n",
       "import org.apache.spark.ml.tuning.CrossValidator\n",
       "import org.apache.spark.ml.Pipeline\n",
       "seed: Int = 5043\n",
       "rf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_7e348110766e\n",
       "rfStages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(featureHasher_c52632897fac, oneHot_84595a7a6062, oneHot_c0f4033f6254, vecAssembler_ec554a896ced, rfr_7e348110766e)\n",
       "rfPipe: org.apache.spark.ml.Pipeline = pipeline_7e4d1..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val seed = 5043\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setMaxBins(4)\n",
    "  .setMaxDepth(2)\n",
    "  .setNumTrees(10)\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "  .setSeed(seed)\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "val rfStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            rf\n",
    ")\n",
    "\n",
    "val rfPipe = new Pipeline().setStages(rfStages) \n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "val rfModel = rfPipe.fit(train)\n",
    "\n",
    "//Make predictions using the model and the test data\n",
    "val rfPredictions = rfModel.transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res19: Array[String] = Array(Price, MethodOfSale, PropertyType, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Suburb, Date, StreetName, str_name_suburb_vec, m_sale_vec, pt_vec, features, Predicted Price)\n"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|   Price|Predicted Price|\n",
      "+--------+---------------+\n",
      "|170000.0|      1043521.0|\n",
      "|280000.0|      1072249.0|\n",
      "|280500.0|       745230.0|\n",
      "|283000.0|       766050.0|\n",
      "|290000.0|      1121635.0|\n",
      "|300000.0|       745230.0|\n",
      "|300000.0|      1107235.0|\n",
      "|305000.0|      1107235.0|\n",
      "|310000.0|      1057849.0|\n",
      "|316000.0|      1107235.0|\n",
      "|320000.0|      1121635.0|\n",
      "|320000.0|      1121635.0|\n",
      "|320000.0|       824513.0|\n",
      "|320000.0|      1121635.0|\n",
      "|325000.0|       784711.0|\n",
      "|333000.0|       745230.0|\n",
      "|340000.0|       766050.0|\n",
      "|345000.0|      1107235.0|\n",
      "|348000.0|      1022987.0|\n",
      "|350000.0|       973601.0|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error RMSE on test data = 434957.1975603789\n"
     ]
    }
   ],
   "source": [
    "evaluate(rfPredictions,\"rmse\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error R2 on test data = 0.5450783935759977\n"
     ]
    }
   ],
   "source": [
    "evaluate(rfPredictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "<span style='color:red'> to do: \n",
    "    \n",
    "    * finish implementation for Cross-validation \n",
    "    * check if finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "java.lang.IllegalArgumentException",
     "evalue": " rfr_c20e56154079 parameter impurity given invalid value entropy.",
     "output_type": "error",
     "traceback": [
      "java.lang.IllegalArgumentException: rfr_c20e56154079 parameter impurity given invalid value entropy.",
      "  at org.apache.spark.ml.param.Param.validate(params.scala:77)",
      "  at org.apache.spark.ml.param.ParamPair.<init>(params.scala:656)",
      "  at org.apache.spark.ml.param.Param.$minus$greater(params.scala:87)",
      "  at org.apache.spark.ml.param.ParamMap.put(params.scala:950)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder$$anonfun$build$1$$anonfun$1$$anonfun$apply$1.apply(ParamGridBuilder.scala:114)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder$$anonfun$build$1$$anonfun$1$$anonfun$apply$1.apply(ParamGridBuilder.scala:114)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)",
      "  at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)",
      "  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)",
      "  at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)",
      "  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder$$anonfun$build$1$$anonfun$1.apply(ParamGridBuilder.scala:114)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder$$anonfun$build$1$$anonfun$1.apply(ParamGridBuilder.scala:113)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.foreach(List.scala:392)",
      "  at scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)",
      "  at scala.collection.immutable.List.flatMap(List.scala:355)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder$$anonfun$build$1.apply(ParamGridBuilder.scala:113)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder$$anonfun$build$1.apply(ParamGridBuilder.scala:112)",
      "  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)",
      "  at scala.collection.mutable.HashMap$$anonfun$foreach$1.apply(HashMap.scala:130)",
      "  at scala.collection.mutable.HashTable$class.foreachEntry(HashTable.scala:236)",
      "  at scala.collection.mutable.HashMap.foreachEntry(HashMap.scala:40)",
      "  at scala.collection.mutable.HashMap.foreach(HashMap.scala:130)",
      "  at org.apache.spark.ml.tuning.ParamGridBuilder.build(ParamGridBuilder.scala:112)",
      "  ... 38 elided",
      ""
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.sql.{ DataFrame, Row, SQLContext }\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "// Models hypoparameters\n",
    "val numTrees = Seq(5,10,15)\n",
    "val maxBins = Seq(2,5,10)\n",
    "val maxDepth = Seq(2,3,5)\n",
    "val impurity = Seq(\"gini\",\"entropy\",\"variance\",)\n",
    "val featureSubsetStrategy = Seq(\"sqrt\")\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "val rfParamMap = new ParamGridBuilder()\n",
    "                      .addGrid(rf.numTrees, numTrees)\n",
    "                      .addGrid(rf.maxDepth, maxDepth)\n",
    "                      .addGrid(rf.impurity, impurity)\n",
    "                      .addGrid(rf.maxBins, maxBins)\n",
    "                      .addGrid(rf.featureSubsetStrategy, featureSubsetStrategy)\n",
    "                      .build()\n",
    "\n",
    "\n",
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "val rfStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            rf\n",
    ")\n",
    "\n",
    "val rfPipeline = new Pipeline()\n",
    "    .setStages(rfStages)\n",
    "\n",
    "val cvRF = new CrossValidator()\n",
    "    .setEstimator(rfPipeline)\n",
    "    .setEvaluator(new RegressionEvaluator()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "    .setMetricName(\"rmse\"))\n",
    "    .setEstimatorParamMaps(rfParamMap)\n",
    "    .setNumFolds(5)\n",
    "    .setParallelism(2)\n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val cvRFModel = cvRF.fit(train)\n",
    "val rFPredictionsAndPrice = cvRFModel\n",
    "    .transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [{p.name: v for p, v in m.items()} for m in cvModel.getEstimatorParamMaps()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val cv = new CrossValidator().setNumFolds(10).setEstimator(pipeline).\n",
    "             setEvaluator(new BinaryClassificationEvaluator)\n",
    "val cmModel = cv.fit(train)\n",
    "\n",
    "val rfCVPredictions = cmModel.transform(test)\n",
    "\n",
    "rfCVPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n",
    "\n",
    "\n",
    "Mean squared error (MSE) is defined as the average of squared differences between the predicted outcome and the true outcome. \n",
    "\n",
    "R2 coefficient represents the proportion of variance in the outcome that our model is capable of predicting based on its features.\n",
    "\n",
    "**Bias vs Variance**\n",
    "Graph of Error (validation error and training error) versus training set size. They should converge\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Apache Spark (n.d.). _Spark ML Programming Guide._ Retrieved from https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "\n",
    "Gorczynski M. (2017). _Introduction to machine learning with spark and mllib (dataframe API)._ Retrieved from https://scalac.io/scala-spark-ml-machine-learning-introduction/\n",
    "\n",
    "Hydrospheredata (2020). _Program creek. Scala Code Examples. Scaler_ Retrieved from https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "Johnson S (2019). _From sckit-learn to Spark ML._ Retrieved from \n",
    "https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852\n",
    "\n",
    "\n",
    "Johnson S (2019). _Housing Prices - Spark ML Project_ Retrieved from https://github.com/scottdjohnson/HousingPricePredictions/blob/master/HousingPrices-SparkML.ipynb\n",
    "\n",
    "Masri A. (2019). _FeatureTransformation._ Retrieved from \n",
    "\n",
    "https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n",
    "\n",
    "\n",
    "Scala Doc (n.d.)  Retrieved from https://docs.scala-lang.org\n",
    "\n",
    "Jen G. (2020) _FeatureHasher._ Retrieved from https://george-jen.gitbook.io/data-science-and-apache-spark/featurehasher\n",
    "\n",
    "\n",
    "(2019) _Random Forest Classifier with Apache Spark_ Retireved from https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.feature.FeatureHasher\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")\n",
    "\n",
    "var df_featured = hasher.transform(df_clean)\n",
    "\n",
    "df_featured = df_featured.drop(\"StreetName\").drop(\"Suburb\")\n",
    "df_featured.select(\"str_name_suburb_vec\").show(false)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
