{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/input/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p  /tmp/input\n",
    "! hadoop fs -put   -p  ./../data-clean/*.csv             /tmp/input         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Check Spark Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(spark.app.id,local-1590899599995)\n",
      "(spark.repl.class.uri,spark://6228915d44c3:43565/classes)\n",
      "(spark.executor.id,driver)\n",
      "(spark.app.name,spylon-kernel)\n",
      "(spark.repl.class.outputDir,/tmp/tmp8sr4d345)\n",
      "(spark.driver.port,43565)\n",
      "(spark.executor.memory,6g)\n",
      "(spark.driver.memory,6g)\n",
      "(spark.rdd.compress,True)\n",
      "(spark.driver.host,6228915d44c3)\n",
      "(spark.serializer.objectStreamReset,100)\n",
      "(spark.master,local[*])\n",
      "(spark.submit.deployMode,client)\n",
      "(spark.ui.showConsoleProgress,true)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.SparkSession\n",
       "import org.apache.spark.{SparkConf, SparkContext}\n",
       "cs: org.apache.spark.SparkConf = org.apache.spark.SparkConf@25941b0\n"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.{SparkConf,SparkContext}\n",
    "\n",
    "val cs = spark.sparkContext.getConf\n",
    "sc.getConf.getAll.foreach { println }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 05:05:22,948 WARN  [Thread-4] execution.CacheManager (Logging.scala:logWarning(66)) - Asked to cache already cached data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n",
       "res31: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load Clean Dataset into a DataFrame from HDFS after wrangling is completed\n",
    "var df_clean = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/input/*.csv\")\n",
    "df_clean.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))\n",
    "    .withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"Double\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"Double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 05:05:23,325 WARN  [Thread-4] execution.CacheManager (Logging.scala:logWarning(66)) - Asked to cache already cached data.\n",
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.cache()\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change attributes into vectors\n",
    "#### Transform Sale Date into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Date\",unix_timestamp($\"Date\", \"dd/mm/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set FeatureHasher for Suburb, StreetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{FeatureHasher, OneHotEncoder, StandardScaler, VectorAssembler}\n",
       "hasher: org.apache.spark.ml.feature.FeatureHasher = featureHasher_2b5057687c1d\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{FeatureHasher,OneHotEncoder,StandardScaler,VectorAssembler}\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OneHotEncoders for PropertyType, MethodOfSale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ms_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_e24d6a3383fa\n",
       "pt_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_80338ee8a80b\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"MethodOfSale\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"PropertyType\")\n",
    "      .setOutputCol(\"pt_vec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the columns and column vectors into a single column - \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Pipeline\n",
       "columns: Array[String] = Array(Price, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec, m_sale_vec, pt_vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_d37f7bf45007\n",
       "stages: Array[org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.Transformer with org.apache.spark.ml.param.shared.HasOutputCol with org.apache.spark.ml.util.DefaultParamsWritable}] = Array(featureHasher_2b5057687c1d, oneHot_e24d6a3383fa, oneHot_80338ee8a80b, vecAssembler_d37f7bf45007)\n",
       "dd: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 10 ..."
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "val stages = Array(\n",
    "        hasher,\n",
    "        ms_encoder, \n",
    "        pt_encoder,\n",
    "        assembler\n",
    "        //scaler,\n",
    "    )\n",
    "\n",
    "val dd = hasher.transform(df_clean).drop(\"StreetName\",\"Suburb\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res33: Array[String] = Array(Price, MethodOfSale, PropertyType, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec)\n"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mm: org.apache.spark.sql.DataFrame = [Price: double, PropertyType: int ... 10 more fields]\n"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mm = ms_encoder.transform(dd).drop(\"MethodOfSale\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res34: Array[String] = Array(Price, PropertyType, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec, m_sale_vec)\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pt: org.apache.spark.sql.DataFrame = [Price: double, DistanceFromCBD: double ... 10 more fields]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val pt = pt_encoder.transform(mm).drop(\"PropertyType\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature_ds: org.apache.spark.sql.DataFrame = [Price: double, features: vector]\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val feature_ds = assembler.transform(pt).drop(\"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+\n",
      "|    Price|            features|\n",
      "+---------+--------------------+\n",
      "|1480000.0|(262164,[0,1,2,3,...|\n",
      "|1035000.0|(262164,[0,1,2,3,...|\n",
      "|1465000.0|(262164,[0,1,2,3,...|\n",
      "| 850000.0|(262164,[0,1,2,3,...|\n",
      "|1600000.0|(262164,[0,1,2,3,...|\n",
      "| 941000.0|(262164,[0,1,2,3,...|\n",
      "|1876000.0|(262164,[0,1,2,3,...|\n",
      "|1636000.0|(262164,[0,1,2,3,...|\n",
      "|1097000.0|(262164,[0,1,2,3,...|\n",
      "|1350000.0|(262164,[0,1,2,3,...|\n",
      "|1172500.0|(262164,[0,1,2,3,...|\n",
      "|1310000.0|(262164,[0,1,2,3,...|\n",
      "|1200000.0|(262164,[0,1,2,3,...|\n",
      "|1176500.0|(262164,[0,1,2,3,...|\n",
      "| 955000.0|(262164,[0,1,2,3,...|\n",
      "| 890000.0|(262164,[0,1,2,3,...|\n",
      "|1330000.0|(262164,[0,1,2,3,...|\n",
      "|1090000.0|(262164,[0,1,2,3,...|\n",
      "|1100000.0|(262164,[0,1,2,3,...|\n",
      "|1315000.0|(262164,[0,1,2,3,...|\n",
      "+---------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_ds.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_644cc86e74db\n"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"scaledFeatures\")\n",
    "      .setWithStd(true).setWithMean(true)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into a Training and a Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])\n"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "     val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "    \n",
    "     (train, test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, features: vector]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, features: vector]\n",
       "res37: test.type = [Price: double, features: vector]\n"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val (train, test) = train_test_split(feature_ds)\n",
    "train.cache()\n",
    "test.cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.Pipeline\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_d044f1232fb8\n"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Predicted Price\")\n",
    "    .setMaxIter(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define time function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "time: [R](block: => R)R\n"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def time[R](block: => R): R = {\n",
    "  val t0 = System.nanoTime()\n",
    "  val result = block    // call-by-name\n",
    "  val t1 = System.nanoTime()\n",
    "  println(\"Elapsed time: \" + (t1 - t0)/1000000000 + \" s\")\n",
    "  result\n",
    " }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define predictions function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Predictor\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.PredictionModel\n",
       "predictions: [R <: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], M <: org.apache.spark.ml.PredictionModel[org.apache.spark.ml.linalg.Vector,M]](predictor: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "\n",
    "\n",
    "def predictions[R <: Predictor[Vector, R, M],\n",
    "                M <: PredictionModel[Vector, M]](\n",
    "    predictor: Predictor[Vector, R, M],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "\n",
    "    val stages = Array(\n",
    "        //scaler,\n",
    "        predictor\n",
    "    )\n",
    "   \n",
    "    val pipeline = new Pipeline()\n",
    "        .setStages(stages)\n",
    "    \n",
    "    val result = pipeline.fit(train).transform(test)\n",
    "    result\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-31 05:05:47,572 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-05-31 05:05:47,573 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "Elapsed time: 103 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "lrPredictions: org.apache.spark.sql.DataFrame = [Price: double, features: vector ... 1 more field]\n",
       "res38: lrPredictions.type = [Price: double, features: vector ... 1 more field]\n"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val lrPredictions = time{predictions(lr, train, test)}\n",
    "lrPredictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res39: Array[String] = Array(Price, features, Predicted Price)\n"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lrPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|   Price|Predicted Price|\n",
      "+--------+---------------+\n",
      "|170000.0|       170169.0|\n",
      "|280000.0|       279875.0|\n",
      "|280500.0|       280500.0|\n",
      "|283000.0|       283641.0|\n",
      "|290000.0|       290089.0|\n",
      "|300000.0|       300006.0|\n",
      "|300000.0|       300107.0|\n",
      "|305000.0|       304987.0|\n",
      "|310000.0|       310302.0|\n",
      "|316000.0|       315656.0|\n",
      "|320000.0|       319992.0|\n",
      "|320000.0|       320061.0|\n",
      "|320000.0|       320772.0|\n",
      "|320000.0|       320588.0|\n",
      "|325000.0|       325022.0|\n",
      "|333000.0|       332987.0|\n",
      "|340000.0|       338789.0|\n",
      "|345000.0|       347193.0|\n",
      "|348000.0|       347882.0|\n",
      "|350000.0|       350000.0|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lrPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "evaluate: (predictions: org.apache.spark.sql.DataFrame, metric: String)Unit\n"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "def evaluate ( predictions: DataFrame, metric: String) = {\n",
    "    val eval =  new RegressionEvaluator()\n",
    "       .setLabelCol(\"Price\")\n",
    "       .setPredictionCol(\"Predicted Price\")\n",
    "       .setMetricName(metric)\n",
    "println(\"Root Mean Squared Error \"+  metric.toUpperCase()+\" on test data = \" + eval.evaluate(predictions))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n",
    "\n",
    "**Mean squared error (MSE)** -- the average of squared differences between the predicted outcome and the true outcome.\n",
    "\n",
    "**R2 coefficient** -- the proportion of variance in the outcome that our model is capable of predicting based on its features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error RMSE on test data = 401.0991912963174\n"
     ]
    }
   ],
   "source": [
    "evaluate(lrPredictions,\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error R2 on test data = 0.9999996131459988\n"
     ]
    }
   ],
   "source": [
    "evaluate(lrPredictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "<span style=\"color:red\">\n",
    "TO DO: does not finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.Predictor\n",
       "import org.apache.spark.ml.linalg.Vector\n",
       "import org.apache.spark.ml.PredictionModel\n",
       "import org.apache.spark.ml.param.ParamMap\n",
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
       "train_eval: [R <: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], M <: org.apache.spark.ml.PredictionModel[org.apache.spark.ml.linalg.Vector,M]](predictor: org.apache.spark.ml.Predictor[org.apache.spark.ml.linalg.Vector,R,M], paramMap: Array[org.apache.spark.ml.param.ParamMap], train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame)org.apache.spark.ml.Model[_]\n"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "\n",
    "\n",
    "\n",
    "def train_eval[R <: Predictor[Vector, R, M],\n",
    "               M <: PredictionModel[Vector, M]](\n",
    "    predictor: Predictor[Vector, R, M],\n",
    "    paramMap: Array[ParamMap],\n",
    "    train: DataFrame, \n",
    "    test: DataFrame) = {\n",
    "\n",
    "    val stages = Array(\n",
    "        //scaler,\n",
    "        predictor\n",
    "    )\n",
    "   \n",
    "    val pipeline = new Pipeline()\n",
    "        .setStages(stages)\n",
    "    \n",
    "    val cv = new CrossValidator()\n",
    "        .setEstimator(pipeline)\n",
    "        .setEvaluator(new RegressionEvaluator()\n",
    "        .setLabelCol(\"Price\")\n",
    "        .setPredictionCol(\"Predicted Price\")\n",
    "        .setMetricName(\"rmse\"))\n",
    "        .setEstimatorParamMaps(paramMap)\n",
    "        .setNumFolds(5)\n",
    "        .setParallelism(2)\n",
    "\n",
    "    val cvModel = cv.fit(train)\n",
    "    val predictions = cvModel.transform(test)\n",
    "    \n",
    "    predictions.cache()\n",
    "    evaluate(predictions,\"rmse\")\n",
    "    evaluate(predictions,\"r2\")\n",
    "    \n",
    "    val bestModel = cvModel.bestModel\n",
    "    \n",
    "    println(bestModel.extractParamMap)\n",
    "    \n",
    "    bestModel\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10,1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0,0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10, 50, 100, 500, 800))\n",
    "    .build()\n",
    "\n",
    "val t0 = System.nanoTime()\n",
    "val bestLRModel = train_eval(lr, lrParamMap, train, test)\n",
    "val t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0)/(1000000000) + \" s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest Regression\n",
    "\n",
    "**Build Random Forest model**\n",
    "Specify maxDepth, maxBins, auto and seed parameters.\n",
    "\n",
    "**maxDepth** -- Maximum depth of a tree. Increasing the depth makes the model more powerful, but deep trees take longer to train.\n",
    "\n",
    "**maxBins** -- Maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node.\n",
    "\n",
    "**auto** -- Automatically select the number of features to consider for splits at each tree node\n",
    "\n",
    "**seed** -- Use a random seed number , allowing to repeat the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If the number of trees is 1, then no bootstrapping is used at all. However, if the number of trees is > 1, then the bootstrapping is accomplished. Where, the parameter featureSubsetStrategy signifies the number of features to be considered for splits at each node. The supported values of featureSubsetStrategy are \"auto\", \"all\", \"sqrt\", \"log2\" and \"on third\". The supported numerical values, on the other hand, are (0.0-1.0] and [1-n]. However, if featureSubsetStrategy is chosen as \"auto\", the algorithm chooses the best feature subset strategy automatically\n",
    "\n",
    "\n",
    "If the numTrees == 1, the featureSubsetStrategy is set to be \"all\". However, if the numTrees > 1 (i.e., forest), featureSubsetStrategy is set to be \"onethird\" for regression\n",
    "\n",
    "\n",
    "Moreover, if a real value \"n\" is in the range (0, 1.0] is set, n*number_of_features is used consequently. However, if an integer value \"n\" is in the range (1, the number of features) is set, only n features are used alternatively\n",
    "\n",
    "\n",
    "The parameter categoricalFeaturesInfo which is a map is used for storing arbitrary of categorical features. An entry (n -> k) indicates that feature n is categorical with k categories indexed from 0: {0, 1,...,k-1}\n",
    "The impurity criterion used for information gain calculation. The supported values are “gini\" and “variance”. The former is the only supported value for classification. The latter is used for regression\n",
    "\n",
    "\n",
    "The maxDepth is the maximum depth of the tree. (e.g., depth 0 means 1 leaf node, depth 1 means 1 internal node + 2 leaf nodes). However, the suggested value is 4 to get a better result\n",
    "\n",
    "\n",
    "The maxBins signifies the maximum number of bins used for splitting the features; where the suggested value is 100 to get better results\n",
    "\n",
    "\n",
    "Finally, the random seed is used for bootstrapping and choosing feature subsets to avoid the random nature of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val seed = 5043\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setMaxBins(100)\n",
    "  .setMaxDepth(4)\n",
    "  .setNumTrees(8)\n",
    "  .setFeatureSubsetStrategy(\"onethird\")\n",
    "  .setSeed(seed)\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val rfPredictions = time{predictions(rf, train, test)}\n",
    "rfPredictions.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rfPredictions,\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rfPredictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "<span style=\"color:red\">\n",
    "TO DO: \n",
    "* finish implementation for Cross-validation \n",
    "* check if finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "\n",
    "// Models hypoparameters\n",
    "val numTrees = Seq(5)//,10,15)\n",
    "val maxBins = Seq(2)//,5,10)\n",
    "val maxDepth = Seq(2)//,3,5)\n",
    "//val impurity = Seq(\"gini\")//,\"entropy\",\"variance\",)\n",
    "val featureSubsetStrategy = Seq(\"sqrt\")\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "\n",
    "val rfParamMap = new ParamGridBuilder()\n",
    "  .addGrid(rf.numTrees, numTrees)\n",
    "  .addGrid(rf.maxDepth, maxDepth)\n",
    "//  .addGrid(rf.impurity, impurity)\n",
    "  .addGrid(rf.maxBins, maxBins)\n",
    "  .addGrid(rf.featureSubsetStrategy, featureSubsetStrategy)\n",
    "  .build()\n",
    "\n",
    "val t0 = System.nanoTime()\n",
    "val best_model = train_eval(rf, lrParamMap, train, test)\n",
    "val t1 = System.nanoTime()\n",
    "println(\"Elapsed time: \" + (t1 - t0)/(1000000000) + \" s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias vs Variance Graph of Error (validation error and training error) versus training set size. \n",
    "\n",
    "\n",
    "<span style=\"color:red\">\n",
    "TO DO: \n",
    "produce graph -- validation error and training error should converge\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Apache Spark (n.d.). Spark ML Programming Guide. Retrieved from https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "\n",
    "Gorczynski M. (2017). Introduction to machine learning with spark and mllib (dataframe API). Retrieved from https://scalac.io/scala-spark-ml-machine-learning-introduction/\n",
    "\n",
    "Hydrospheredata (2020). Program creek. Scala Code Examples. Scaler Retrieved from https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n",
    "\n",
    "Jen G. (2020) FeatureHasher. Retrieved from https://george-jen.gitbook.io/data-science-and-apache-spark/featurehasher\n",
    "\n",
    "Johnson S (2019). From sckit-learn to Spark ML. Retrieved from https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852\n",
    "\n",
    "Johnson S (2019). Housing Prices - Spark ML Project Retrieved from https://github.com/scottdjohnson/HousingPricePredictions/blob/master/HousingPrices-SparkML.ipynb\n",
    "\n",
    "Masri A. (2019). FeatureTransformation. Retrieved from\n",
    "https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n",
    "\n",
    "Scala Doc (n.d.) Retrieved from https://docs.scala-lang.org\n",
    "\n",
    "\n",
    "(2019) Random Forest Classifier with Apache Spark Retireved from https://medium.com/rahasak/random-forest-classifier-with-apache-spark-c63b4a23a7cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
