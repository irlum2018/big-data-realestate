{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "put: `/tmp/input/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p  /tmp/input\n",
    "! hadoop fs -put   -p  ./../data-clean/*.csv             /tmp/input         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Load Clean Dataset into a DataFrame from HDFS after wrangling is completed\n",
    "var df_clean = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/input/*.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))\n",
    "    .withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"Double\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"Double\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: double (nullable = true)\n",
      " |-- Longtitude: double (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change attributes into vectors\n",
    "#### Transform Sale Date into a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_clean: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_clean = df_clean.withColumn(\"Date\",unix_timestamp($\"Date\", \"dd/mm/yyyy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set FeatureHasher for Suburb, StreetName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.{FeatureHasher, OneHotEncoder, StandardScaler, VectorAssembler}\n",
       "hasher: org.apache.spark.ml.feature.FeatureHasher = featureHasher_5735c63a7d27\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.{FeatureHasher,OneHotEncoder,StandardScaler,VectorAssembler}\n",
    "\n",
    "val hasher = new FeatureHasher()\n",
    " .setInputCols(\"StreetName\",\"Suburb\")\n",
    " .setOutputCol(\"str_name_suburb_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set OneHotEncoders for PropertyType, MethodOfSale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ms_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_a1f3c99429b9\n",
       "pt_encoder: org.apache.spark.ml.feature.OneHotEncoder = oneHot_5843e222abf3\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val ms_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"MethodOfSale\")\n",
    "      .setOutputCol(\"m_sale_vec\")\n",
    "\n",
    "val pt_encoder = new OneHotEncoder()\n",
    "      .setInputCol(\"PropertyType\")\n",
    "      .setOutputCol(\"pt_vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assemble the columns and column vectors into a single column - \"features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "columns: Array[String] = Array(Price, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Date, str_name_suburb_vec, m_sale_vec, pt_vec)\n",
       "assembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_8a045747b379\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scaler: org.apache.spark.ml.feature.StandardScaler = stdScal_0e120a6494eb\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var scaler = new StandardScaler()\n",
    "      .setInputCol(\"features\")\n",
    "      .setOutputCol(\"ScaledFeatures\")\n",
    "      .setWithStd(true).setWithMean(true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split Data into a Training and a Testing Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.feature.VectorAssembler\n",
       "import org.apache.spark.sql.DataFrame\n",
       "import org.apache.spark.sql.functions._\n",
       "train_test_split: (data: org.apache.spark.sql.DataFrame)(org.apache.spark.sql.Dataset[org.apache.spark.sql.Row], org.apache.spark.sql.Dataset[org.apache.spark.sql.Row])\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.feature.VectorAssembler\n",
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "\n",
    "def train_test_split(data: DataFrame) = {\n",
    "    \n",
    "     val Array(train, test) = data.randomSplit(Array(0.8, 0.2), seed = 30)\n",
    "    \n",
    "     (train, test)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, MethodOfSale: int ... 11 more fields]\n",
       "test: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "val (train, test) = train_test_split(df_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Apply Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.regression.LinearRegression\n",
       "import org.apache.spark.ml.Pipeline\n",
       "lr: org.apache.spark.ml.regression.LinearRegression = linReg_9e079f937458\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Predicted Price\")\n",
    "    .setMaxIter(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Stages to transform all columns into feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lrStages: Array[org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable{def copy(extra: org.apache.spark.ml.param.ParamMap): org.apache.spark.ml.PipelineStage with org.apache.spark.ml.util.DefaultParamsWritable}}] = Array(featureHasher_5735c63a7d27, oneHot_a1f3c99429b9, oneHot_5843e222abf3, vecAssembler_8a045747b379, linReg_9e079f937458)\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// add linear regression to stages\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Construct the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-30 03:46:52,237 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n",
      "2020-05-30 03:46:52,237 WARN  [Thread-4] netlib.BLAS (BLAS.java:<clinit>(61)) - Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\n",
      "pipeline was executed 115"
     ]
    },
    {
     "data": {
      "text/plain": [
       "startTimeMillis: Long = 1590810400873\n",
       "lrPipe: org.apache.spark.ml.Pipeline = pipeline_342801117ebd\n",
       "lrModel: org.apache.spark.ml.PipelineModel = pipeline_342801117ebd\n",
       "predictions: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 16 more fields]\n",
       "endTimeMillis: Long = 1590810516511\n",
       "durationSeconds: Long = 115\n",
       "res3: Int = 20\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val lrPipe = new Pipeline().setStages(lrStages)\n",
    "\n",
    "//We fit our DataFrame into the pipeline to generate a model\n",
    "val lrModel = lrPipe.fit(train)\n",
    "\n",
    "//Make predictions using the model and the test data\n",
    "val predictions = lrModel.transform(test)\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)\n",
    "20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "res4: Array[String] = Array(Price, MethodOfSale, PropertyType, DistanceFromCBD, Rooms, Bathroom, Car, Landsize, Latitude, Longtitude, Suburb, Date, StreetName, str_name_suburb_vec, m_sale_vec, pt_vec, features, Predicted Price)\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------------+\n",
      "|   Price|Predicted Price|\n",
      "+--------+---------------+\n",
      "|170000.0|       170064.0|\n",
      "|280000.0|       280013.0|\n",
      "|280500.0|       280579.0|\n",
      "|283000.0|       283663.0|\n",
      "|290000.0|       290036.0|\n",
      "|300000.0|       300055.0|\n",
      "|300000.0|       299881.0|\n",
      "|305000.0|       305248.0|\n",
      "|310000.0|       310135.0|\n",
      "|316000.0|       315935.0|\n",
      "|320000.0|       320027.0|\n",
      "|320000.0|       320035.0|\n",
      "|320000.0|       320560.0|\n",
      "|320000.0|       319303.0|\n",
      "|325000.0|       325090.0|\n",
      "|333000.0|       332989.0|\n",
      "|340000.0|       342929.0|\n",
      "|345000.0|       345065.0|\n",
      "|348000.0|       348240.0|\n",
      "|350000.0|       350108.0|\n",
      "+--------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
       "evaluate: (predictions: org.apache.spark.sql.DataFrame, metric: String)Unit\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "\n",
    "def evaluate ( predictions: DataFrame, metric: String) = {\n",
    "    val eval =  new RegressionEvaluator()\n",
    "       .setLabelCol(\"Price\")\n",
    "       .setPredictionCol(\"Predicted Price\")\n",
    "       .setMetricName(metric)\n",
    "println(\"Root Mean Squared Error \"+  metric.toUpperCase()+\" on test data = \" + eval.evaluate(predictions))\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression metrics\n",
    "\n",
    "**Mean squared error (MSE)** -- the average of squared differences between the predicted outcome and the true outcome.\n",
    "\n",
    "**R2 coefficient** -- the proportion of variance in the outcome that our model is capable of predicting based on its features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error RMSE on test data = 975.1264181243374\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions,\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error R2 on test data = 0.9999977135307482\n"
     ]
    }
   ],
   "source": [
    "evaluate(predictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "\n",
    "<span style=\"color:red\">\n",
    "TO DO: does not finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "\n",
    "val lr = new LinearRegression()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "val lrStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            lr\n",
    ")\n",
    "\n",
    "val lrPipeline = new Pipeline()\n",
    "    .setStages(lrStages)\n",
    "\n",
    "val cvLR = new CrossValidator()\n",
    "    .setEstimator(lrPipeline)\n",
    "    .setEvaluator(new RegressionEvaluator()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "    .setMetricName(\"rmse\"))\n",
    "    .setEstimatorParamMaps(lrParamMap)\n",
    "    .setNumFolds(5)\n",
    "    .setParallelism(2)\n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "\n",
    "val cvLRModel = cvLR.fit(train)\n",
    "val lrPredictionsAndPrice = cvLRModel\n",
    "    .transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictionsAndPrice.show()\n",
    "val bestLRModel = cvLRModel.bestModel\n",
    "    \n",
    "println(bestLRModel.extractParamMap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrPredictionsAndPrice.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Apply KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "\n",
    "\n",
    "Pipeline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation\n",
    "\n",
    "Pipeline Model Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Apply Random Forest Regression\n",
    "\n",
    "**Build Random Forest model**\n",
    "Specify maxDepth, maxBins, impurity, auto and seed parameters.\n",
    "\n",
    "**maxDepth** -- Maximum depth of a tree. Increasing the depth makes the model more powerful, but deep trees take longer to train.\n",
    "\n",
    "**maxBins** -- Maximum number of bins used for discretizing continuous features and for choosing how to split on features at each node.\n",
    "\n",
    "**impurity** -- Criterion used for information gain calculation\n",
    "\n",
    "**auto** -- Automatically select the number of features to consider for splits at each tree node\n",
    "\n",
    "**seed** -- Use a random seed number , allowing to repeat the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-30 03:52:06,525 WARN  [Executor task launch worker for task 76] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_186_0 in memory! (computed 272.9 MB so far)\n",
      "2020-05-30 03:52:06,531 WARN  [Executor task launch worker for task 76] storage.BlockManager (Logging.scala:logWarning(66)) - Persisting block rdd_186_0 to disk instead.\n",
      "2020-05-30 03:52:45,388 WARN  [Executor task launch worker for task 76] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_186_0 in memory! (computed 272.9 MB so far)\n",
      "2020-05-30 03:54:18,239 WARN  [Executor task launch worker for task 78] memory.MemoryStore (Logging.scala:logWarning(66)) - Not enough space to cache rdd_186_0 in memory! (computed 272.9 MB so far)\n",
      "2020-05-30 03:54:19,883 ERROR [Executor task launch worker for task 78] executor.Executor (Logging.scala:logError(91)) - Exception in task 0.0 in stage 78.0 (TID 78)\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
      "\tat scala.Array$.tabulate(Array.scala:331)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2020-05-30 03:54:19,904 ERROR [Executor task launch worker for task 78] util.SparkUncaughtExceptionHandler (Logging.scala:logError(91)) - Uncaught exception in thread Thread[Executor task launch worker for task 78,5,main]\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
      "\tat scala.Array$.tabulate(Array.scala:331)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "2020-05-30 03:54:19,909 WARN  [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logWarning(66)) - Lost task 0.0 in stage 78.0 (TID 78, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
      "\tat scala.Array$.tabulate(Array.scala:331)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "2020-05-30 03:54:19,910 ERROR [task-result-getter-2] scheduler.TaskSetManager (Logging.scala:logError(70)) - Task 0 in stage 78.0 failed 1 times; aborting job\n",
      "2020-05-30 03:54:19,922 ERROR [Thread-4] util.Instrumentation (Logging.scala:logError(70)) - org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 78.0 failed 1 times, most recent failure: Lost task 0.0 in stage 78.0 (TID 78, localhost, executor driver): java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
      "\tat scala.Array$.tabulate(Array.scala:331)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\n",
      "\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\n",
      "\tat scala.Option.foreach(Option.scala:257)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:990)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:743)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions$$anonfun$collectAsMap$1.apply(PairRDDFunctions.scala:742)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n",
      "\tat org.apache.spark.rdd.PairRDDFunctions.collectAsMap(PairRDDFunctions.scala:742)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.findBestSplits(RandomForest.scala:567)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:201)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor$$anonfun$train$1.apply(RandomForestRegressor.scala:133)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor$$anonfun$train$1.apply(RandomForestRegressor.scala:119)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:185)\n",
      "\tat scala.util.Try$.apply(Try.scala:192)\n",
      "\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:185)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:119)\n",
      "\tat org.apache.spark.ml.regression.RandomForestRegressor.train(RandomForestRegressor.scala:46)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n",
      "\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:82)\n",
      "\tat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:153)\n",
      "\tat org.apache.spark.ml.Pipeline$$anonfun$fit$2.apply(Pipeline.scala:149)\n",
      "\tat scala.collection.Iterator$class.foreach(Iterator.scala:891)\n",
      "\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1334)\n",
      "\tat scala.collection.IterableViewLike$Transformed$class.foreach(IterableViewLike.scala:44)\n",
      "\tat scala.collection.SeqViewLike$AbstractTransformed.foreach(SeqViewLike.scala:37)\n",
      "\tat org.apache.spark.ml.Pipeline.fit(Pipeline.scala:149)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:76)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:104)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:106)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:108)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw$$iw$$iw.<init>(<console>:110)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw$$iw.<init>(<console>:112)\n",
      "\tat $line36.$read$$iw$$iw$$iw$$iw.<init>(<console>:114)\n",
      "\tat $line36.$read$$iw$$iw$$iw.<init>(<console>:116)\n",
      "\tat $line36.$read$$iw$$iw.<init>(<console>:118)\n",
      "\tat $line36.$read$$iw.<init>(<console>:120)\n",
      "\tat $line36.$read.<init>(<console>:122)\n",
      "\tat $line36.$read$.<init>(<console>:126)\n",
      "\tat $line36.$read$.<clinit>(<console>)\n",
      "\tat $line36.$eval$.$print$lzycompute(<console>:7)\n",
      "\tat $line36.$eval$.$print(<console>:6)\n",
      "\tat $line36.$eval.$print(<console>)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n",
      "\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n",
      "\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n",
      "\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n",
      "\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n",
      "\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n",
      "\tat sun.reflect.GeneratedMethodAccessor16.invoke(Unknown Source)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tat java.lang.Thread.run(Thread.java:748)\n",
      "Caused by: java.lang.OutOfMemoryError: Java heap space\n",
      "\tat org.apache.spark.ml.tree.impl.DTStatsAggregator.<init>(DTStatsAggregator.scala:77)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:545)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13$$anonfun$14.apply(RandomForest.scala:541)\n",
      "\tat scala.Array$.tabulate(Array.scala:331)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:541)\n",
      "\tat org.apache.spark.ml.tree.impl.RandomForest$$anonfun$13.apply(RandomForest.scala:538)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$23.apply(RDD.scala:823)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:346)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:310)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.ml.regression.RandomForestRegressor\n",
    "import org.apache.spark.ml.tuning.CrossValidator\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "val seed = 5043\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setMaxBins(4)\n",
    "  .setMaxDepth(2)\n",
    "  .setNumTrees(10)\n",
    "  .setFeatureSubsetStrategy(\"auto\")\n",
    "  .setSeed(seed)\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "val rfStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            rf\n",
    ")\n",
    "\n",
    "val rfPipe = new Pipeline().setStages(rfStages) \n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "val rfModel = rfPipe.fit(train)\n",
    "\n",
    "//Make predictions using the model and the test data\n",
    "val rfPredictions = rfModel.transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rfPredictions,\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(rfPredictions,\"r2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing/Evaluation/ Parameter Tuning\n",
    "\n",
    "Cross-validation\n",
    "<span style=\"color:red\">\n",
    "TO DO: \n",
    "* finish implementation for Cross-validation \n",
    "* check if finish run in reasonable time\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.sql.{ DataFrame, Row, SQLContext }\n",
    "import org.apache.spark.ml.feature.{VectorAssembler, StandardScaler}\n",
    "import org.apache.spark.ml.Predictor\n",
    "import org.apache.spark.ml.PredictionModel\n",
    "import org.apache.spark.ml.linalg.Vector\n",
    "import org.apache.spark.ml.evaluation.RegressionEvaluator\n",
    "import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\n",
    "import org.apache.spark.ml.param.ParamMap\n",
    "import org.apache.spark.ml.regression.LinearRegression\n",
    "import org.apache.spark.ml.Pipeline\n",
    "\n",
    "\n",
    "// Models hypoparameters\n",
    "val numTrees = Seq(5,10,15)\n",
    "val maxBins = Seq(2,5,10)\n",
    "val maxDepth = Seq(2,3,5)\n",
    "val impurity = Seq(\"gini\",\"entropy\",\"variance\",)\n",
    "val featureSubsetStrategy = Seq(\"sqrt\")\n",
    "\n",
    "val rf = new RandomForestRegressor()\n",
    "  .setLabelCol(\"Price\")\n",
    "  .setFeaturesCol(\"features\")\n",
    "  .setPredictionCol(\"Predicted Price\")\n",
    "\n",
    "\n",
    "val lrParamMap = new ParamGridBuilder()\n",
    "    .addGrid(lr.regParam, Array(10, 1, 0.1, 0.01, 0.001))\n",
    "    .addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0))\n",
    "    .addGrid(lr.maxIter, Array(10000, 250000))\n",
    "    .build()\n",
    "\n",
    "val rfParamMap = new ParamGridBuilder()\n",
    "                      .addGrid(rf.numTrees, numTrees)\n",
    "                      .addGrid(rf.maxDepth, maxDepth)\n",
    "                      .addGrid(rf.impurity, impurity)\n",
    "                      .addGrid(rf.maxBins, maxBins)\n",
    "                      .addGrid(rf.featureSubsetStrategy, featureSubsetStrategy)\n",
    "                      .build()\n",
    "\n",
    "\n",
    "val columns = Array(\"Price\", \"DistanceFromCBD\", \"Rooms\", \"Bathroom\", \"Car\", \"Landsize\", \"Latitude\", \"Longtitude\", \"Date\", \n",
    "                    \"str_name_suburb_vec\", \"m_sale_vec\", \"pt_vec\")\n",
    "\n",
    "val assembler = new VectorAssembler()\n",
    "            .setInputCols(columns)\n",
    "            .setOutputCol(\"features\")\n",
    "\n",
    "\n",
    "val rfStages = Array(\n",
    "            hasher,\n",
    "            ms_encoder, \n",
    "            pt_encoder,\n",
    "            assembler,\n",
    "            //scaler,\n",
    "            rf\n",
    ")\n",
    "\n",
    "val rfPipeline = new Pipeline()\n",
    "    .setStages(rfStages)\n",
    "\n",
    "val cvRF = new CrossValidator()\n",
    "    .setEstimator(rfPipeline)\n",
    "    .setEvaluator(new RegressionEvaluator()\n",
    "    .setLabelCol(\"Price\")\n",
    "    .setPredictionCol(\"Prediction\")\n",
    "    .setMetricName(\"rmse\"))\n",
    "    .setEstimatorParamMaps(rfParamMap)\n",
    "    .setNumFolds(5)\n",
    "    .setParallelism(2)\n",
    "\n",
    "val startTimeMillis = System.currentTimeMillis()\n",
    "val cvRFModel = cvRF.fit(train)\n",
    "val rFPredictionsAndPrice = cvRFModel\n",
    "    .transform(test)\n",
    "\n",
    "\n",
    "val endTimeMillis = System.currentTimeMillis()\n",
    "val durationSeconds = (endTimeMillis - startTimeMillis) / 1000\n",
    "print(\"pipeline was executed \"+durationSeconds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfPredictions.withColumn(\"Predicted Price\", round($\"Predicted Price\", 0)).select(\"Price\",\"Predicted Price\").show()\n",
    "\n",
    "// this will add new columns rawPrediction, probability and prediction\n",
    "val predictionDf = randomForestModel.transform(testData)\n",
    "predictionDf.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bias vs Variance Graph of Error (validation error and training error) versus training set size. \n",
    "\n",
    "\n",
    "<span style=\"color:red\">\n",
    "TO DO: \n",
    "produce graph -- validation error and training errorshould converge\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "Apache Spark (n.d.). _Spark ML Programming Guide._ Retrieved from https://spark.apache.org/docs/1.2.2/ml-guide.html\n",
    "\n",
    "Hydrospheredata (n.d.). _org.apache.spark.ml.feature.StandardScaler Scala Examples._ Retrieved from https://towardsdatascience.com/from-scikit-learn-to-spark-ml-f2886fb46852\n",
    "\n",
    "Johnson S (2019). _From sckit-learn to Spark ML._ Retrieved from https://www.programcreek.com/scala/org.apache.spark.ml.feature.StandardScaler\n",
    "Masri A. (2019). _FeatureTransformation._ Retrieved from https://towardsdatascience.com/apache-spark-mllib-tutorial-7aba8a1dce6e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
