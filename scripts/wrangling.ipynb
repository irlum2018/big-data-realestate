{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "using magic\n",
    "\n",
    "create input folder on HDFS if not exists\n",
    "copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/big-data-realestate-master/scripts\n",
      "\n",
      "put: `/tmp/rs_input/raw.csv': File exists\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "-rwxrwxrwx   1 1000 staff    5018236 2020-05-22 06:09 /tmp/rs_input/raw.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "! hadoop fs -mkdir -p  /tmp/rs_input\n",
    "! hadoop fs -put   -p  ./../data-raw/Melbourne_housing_FULL.csv             /tmp/rs_input/raw.csv\n",
    "! hadoop fs -ls        /tmp/rs_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_raw: org.apache.spark.sql.DataFrame = [Suburb: string, Address: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//load raw into df\n",
    "val df_raw = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/rs_input/raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, Method: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//suggest better name\n",
    "//only select columns we need now\n",
    "var df_working= df_raw.select(\"Price\",\n",
    "                          \"Method\",\n",
    "                          \"Type\",\n",
    "                          \"Distance\",\n",
    "                          \"Rooms\",\n",
    "                          \"Bathroom\",\n",
    "                          \"Car\",\n",
    "                          \"Landsize\",\n",
    "                          \"Propertycount\",\n",
    "                          \"Suburb\",\n",
    "                          \"Address\",\n",
    "                          \"Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Remove \"#n/a\" records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, Method: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//when profiling there are a number of columns with a \"#N/A\"\n",
    "df_working = df_working.filter(($\"Distance\" =!= \"#N/A\")||($\"Propertycount\" =!= \"#N/A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, Method: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//refactored to remove the for column loop\n",
    "df_working = df_working.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"Distance\",col(\"Distance\").cast(\"Double\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Double\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Propertycount\",col(\"Propertycount\").cast(\"Int\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Address on Street and Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, Method: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//split address on Street\n",
    "df_working = df_working.withColumn(\"StreetName\",split(col(\"Address\"),\" \").getItem(1)).\n",
    "    withColumn(\"StreetSuffix\",split(col(\"Address\"),\" \").getItem(2)).drop(\"Address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, Method: string ... 11 more fields]\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, Method: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// make first letter of suburb upper case\n",
    "df_working= df_working.withColumn(\"Suburb\", initcap(col(\"Suburb\")))\n",
    "\n",
    "//make type code upper\n",
    "df_working = df_working.withColumn(\"Type\", initcap(col(\"Type\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_not_null: org.apache.spark.sql.DataFrame = [Price: double, Method: string ... 11 more fields]\n",
       "res1: Long = 17701\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_not_null = df_working.na.drop()\n",
    "df_not_null.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write down clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_output: Unit = ()\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_output = df_not_null\n",
    "   .coalesce(1)\n",
    "   .write\n",
    "   .format(\"csv\")\n",
    "   .option(\"header\",\"true\")\n",
    "   .mode(\"overwrite\").option(\"sep\",\",\")\n",
    "   .save(\"hdfs://localhost:9000/tmp/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clean data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `./../data-clean/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p /tmp/output\n",
    "! hadoop fs -copyToLocal /tmp/output/\\*.csv ./../data-clean/cleanMelbourneData.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Apache Spark (n.d.). _Spark Scala API (Scaladoc). Overview._ https://spark.apache.org/docs/latest/api/java/overview-summary.html\n",
    "\n",
    "Apache Spark (n.d.). _Basic Statistic._ https://spark.apache.org/docs/latest/ml-statistics.html\n",
    "\n",
    "Bahadoor N. (2020). _Spark Tutorials_ https://allaboutscala.com/big-data/spark/#dataframe-statistics-correlation\n",
    "\n",
    "Databricks. (2020). _Introduction to DataFrames - Scala._  https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html \n",
    "\n",
    "Grimaldi E. (2018). _Pandas vs. Spark: how to handle dataframes (Part II.)_  https://towardsdatascience.com/python-pandas-vs-scala-how-to-handle-dataframes-part-ii-d3e5efe8287d \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
