{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://f731bc560f02:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590541472965)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sandpit/big-data-realestate/scripts\n",
      "\n",
      "put: `/tmp/rs_input/raw.csv': File exists\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "-rw-r--r--   1 root root    5018236 2020-05-26 00:56 /tmp/rs_input/raw.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "! hadoop fs -mkdir -p  /tmp/rs_input\n",
    "! hadoop fs -put   -p  ./../data-raw/Melbourne_housing_FULL.csv             /tmp/rs_input/raw.csv\n",
    "! hadoop fs -ls        /tmp/rs_input/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_raw: org.apache.spark.sql.DataFrame = [Suburb: string, Address: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//load raw into df\n",
    "val df_raw = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/rs_input/raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//only select columns we need now\n",
    "var df_working= df_raw.select(\"Price\",\n",
    "                          \"Method\",\n",
    "                          \"Type\",\n",
    "                          \"Distance\",\n",
    "                          \"Rooms\",\n",
    "                          \"Bathroom\",\n",
    "                          \"Car\",\n",
    "                          \"Landsize\",\n",
    "                          \"Lattitude\",\n",
    "                          \"Longtitude\",    \n",
    "                          \"Suburb\",\n",
    "                          \"Address\",\n",
    "                          \"Date\")\n",
    "\n",
    "\n",
    "//add meaningful to column names\n",
    "df_working = df_working.withColumnRenamed(\"Method\",\"MethodOfSale\")\n",
    "    .withColumnRenamed(\"Distance\",\"DistanceFromCBD\")\n",
    "    .withColumnRenamed(\"Type\",\"PropertyType\")\n",
    "    .withColumnRenamed(\"Lattitude\",\"Latitude\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Remove \"#N/A\" records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//when profiling there are a number of columns with a \"#N/A\" which need to be removed\n",
    "df_working = df_working.filter($\"DistanceFromCBD\" =!= \"#N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change type of columns \"Price\", \"DistanceFromCBD\" & \"Landsize\" to Double, \"Rooms\", \"Bathroom\", \"Car\" to Int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//refactored to remove the for column loop\n",
    "df_working = df_working.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Int\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"Latitude\",col(\"Latitude\").cast(\"String\"))\n",
    "    .withColumn(\"Longtitude\",col(\"Longtitude\").cast(\"String\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Deal with the categorical values for Type and Method of sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//convert categorically values to ints\n",
    "//make sure the categorical type is upper\n",
    "df_working = df_working.withColumn(\"PropertyType\", upper(col(\"PropertyType\")))\n",
    "\n",
    "df_working = df_working.withColumn(\"PropertyType\",\n",
    "when(col(\"PropertyType\") === \"H\", \"1\")\n",
    ".when(col(\"PropertyType\") === \"U\", \"2\")\n",
    ".when(col(\"PropertyType\") ===\"T\", \"3\")\n",
    ".otherwise(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//convert categorically values to ints\n",
    "//make sure the categorical type is upper\n",
    "df_working = df_working.withColumn(\"MethodOfSale\", upper(col(\"MethodOfSale\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_working = df_working.withColumn(\"MethodOfSale\",\n",
    "when(col(\"MethodOfSale\") === \"S\", \"1\")\n",
    ".when(col(\"MethodOfSale\") === \"SP\", \"2\")\n",
    ".when(col(\"MethodOfSale\") === \"PI\", \"3\")\n",
    ".when(col(\"MethodOfSale\") === \"PN\", \"4\")\n",
    ".when(col(\"MethodOfSale\") === \"SN\", \"5\")\n",
    ".when(col(\"MethodOfSale\") === \"VB\", \"6\")\n",
    ".when(col(\"MethodOfSale\") === \"W\", \"7\")\n",
    ".when(col(\"MethodOfSale\") === \"SA\", \"8\")\n",
    ".when(col(\"MethodOfSale\") === \"SS\", \"9\")                                \n",
    ".otherwise(\"0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//cast categorical values to Ints\n",
    "df_working = df_working.withColumn(\"PropertyType\",col(\"PropertyType\").cast(\"Int\"))\n",
    "    .withColumn(\"MethodOfSale\",col(\"MethodOfSale\").cast(\"Int\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make First Letter of Suburb Upper Case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// make first letter of suburb upper case\n",
    "df_working= df_working.withColumn(\"Suburb\", initcap(col(\"Suburb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Address on Street and Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 12 more fields]\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//split address on Street name and Suffix\n",
    "df_working = df_working.withColumn(\"StreetName\",split(col(\"Address\"),\" \").getItem(1)).\n",
    "    withColumn(\"StreetSuffix\",split(col(\"Address\"),\" \").getItem(2)).drop(\"Address\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fix addresses which are \"The something\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 12 more fields]\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 12 more fields]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//fix \"The Parade, The *** adddresses\"\n",
    "df_working = df_working.withColumn(\"StreetName\",\n",
    "when(col(\"StreetName\").like(\"The\"), concat(lit(\"The \"),col(\"StreetSuffix\")))\n",
    ".otherwise(col(\"StreetName\")))\n",
    "\n",
    "//remove the street suffix\n",
    "df_working = df_working.withColumn(\"StreetSuffix\",\n",
    "when(col(\"StreetName\").contains(\"The\"), lit(\"\"))\n",
    ".otherwise(col(\"StreetSuffix\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//Rebuild street name from cleaned tokens\n",
    "//this is approach was due to the legacy code we already had tokenizing the address so i just rejoined them\n",
    "//one could write a regex replace to remove the street numbers unit numbers etc. bu i know these columns are clean\n",
    "df_working = df_working.withColumn(\"StreetName\", concat(col(\"StreetName\"), lit(\" \"),col(\"StreetSuffix\"))).drop(\"StreetSuffix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make First Letter of Street upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// make first letter of Street upper case\n",
    "df_working= df_working.withColumn(\"StreetName\", initcap(col(\"StreetName\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove Bad Landsize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//drop all properties with land area less than 12 sqm \n",
    "df_working = df_working.filter(!($\"Landsize\"<12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//drop rows where type = h and landsize < 50 sqm\n",
    "df_working = df_working.filter(!($\"Landsize\"<50 && $\"PropertyType\"===1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_not_null: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: int ... 11 more fields]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_not_null = df_working.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Price: double (nullable = true)\n",
      " |-- MethodOfSale: integer (nullable = true)\n",
      " |-- PropertyType: integer (nullable = true)\n",
      " |-- DistanceFromCBD: double (nullable = true)\n",
      " |-- Rooms: integer (nullable = true)\n",
      " |-- Bathroom: integer (nullable = true)\n",
      " |-- Car: integer (nullable = true)\n",
      " |-- Landsize: double (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longtitude: string (nullable = true)\n",
      " |-- Suburb: string (nullable = true)\n",
      " |-- Date: string (nullable = true)\n",
      " |-- StreetName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_not_null.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write Down Clean Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_output: Unit = ()\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_output = df_not_null.coalesce(1)\n",
    "   .write\n",
    "   .format(\"csv\")\n",
    "   .option(\"header\",\"true\")\n",
    "   .mode(\"overwrite\").option(\"sep\",\",\")\n",
    "   .save(\"hdfs://localhost:9000/tmp/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the Clean Data to Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `./../data-clean/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p /tmp/output\n",
    "! hadoop fs -copyToLocal /tmp/output/\\*.csv ./../data-clean/cleanMelbourneData.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Apache Spark (n.d.). _Spark Scala API (Scaladoc). Overview._ https://spark.apache.org/docs/latest/api/java/overview-summary.html\n",
    "\n",
    "Databricks. (2020). _Introduction to DataFrames - Scala._  https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html \n",
    "\n",
    "Grimaldi E. (2018). _Pandas vs. Spark: how to handle dataframes (Part II.)_  https://towardsdatascience.com/python-pandas-vs-scala-how-to-handle-dataframes-part-ii-d3e5efe8287d \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
