{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing HDFS\n",
    "Using magic\n",
    "\n",
    "Create input folder on HDFS if not exists\n",
    "\n",
    "Copy from data from local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intitializing Scala interpreter ..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Spark Web UI available at http://4550f27d0b98:4040\n",
       "SparkContext available as 'sc' (version = 2.4.5, master = local[*], app id = local-1590143462072)\n",
       "SparkSession available as 'spark'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-22 10:31:00,289 WARN  [main] util.NativeCodeLoader (NativeCodeLoader.java:<clinit>(62)) - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "import org.apache.spark.sql.functions._\n"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.functions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/big-data-realestate-master/scripts\n",
      "\n",
      "put: `/tmp/rs_input/raw.csv': File exists\n",
      "\n",
      "\n",
      "Found 1 items\n",
      "\n",
      "\n",
      "-rwxrwxrwx   1 1000 staff    5018236 2020-05-22 06:09 /tmp/rs_input/raw.csv\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "! hadoop fs -mkdir -p  /tmp/rs_input\n",
    "! hadoop fs -put   -p  ./../data-raw/Melbourne_housing_FULL.csv             /tmp/rs_input/raw.csv\n",
    "! hadoop fs -ls        /tmp/rs_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_raw: org.apache.spark.sql.DataFrame = [Suburb: string, Address: string ... 19 more fields]\n"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//load raw into df\n",
    "val df_raw = spark\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(\"hdfs://localhost:9000/tmp/rs_input/raw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 10 more fields]\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//only select columns we need now\n",
    "var df_working= df_raw.select(\"Price\",\n",
    "                          \"Method\",\n",
    "                          \"Type\",\n",
    "                          \"Distance\",\n",
    "                          \"Rooms\",\n",
    "                          \"Bathroom\",\n",
    "                          \"Car\",\n",
    "                          \"Landsize\",\n",
    "                          \"Propertycount\",\n",
    "                          \"Suburb\",\n",
    "                          \"Address\",\n",
    "                          \"Date\")\n",
    "\n",
    "\n",
    "//add meaningful to column names\n",
    "df_working = df_working.withColumnRenamed(\"Method\",\"MethodOfSale\")\n",
    "    .withColumnRenamed(\"Distance\",\"DistanceFromCBD\")\n",
    "    .withColumnRenamed(\"Type\",\"PropertyType\")\n",
    "    .withColumnRenamed(\"Propertycount\",\"PropertyCountInSuburb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change Remove \"#n/a\" records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: string, MethodOfSale: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//when profiling there are a number of columns with a \"#N/A\" which need to be removed\n",
    "df_working = df_working.filter(($\"DistanceFromCBD\" =!= \"#N/A\")||($\"PropertyCountInSuburb\" =!= \"#N/A\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//refactored to remove the for column loop\n",
    "df_working = df_working.withColumn(\"Price\",col(\"Price\").cast(\"Double\"))\n",
    "    .withColumn(\"Rooms\",col(\"Rooms\").cast(\"Int\"))\n",
    "    .withColumn(\"DistanceFromCBD\",col(\"DistanceFromCBD\").cast(\"Double\"))\n",
    "    .withColumn(\"Bathroom\",col(\"Bathroom\").cast(\"Double\"))\n",
    "    .withColumn(\"Car\",col(\"Car\").cast(\"Int\"))\n",
    "    .withColumn(\"Landsize\",col(\"Landsize\").cast(\"Double\"))\n",
    "    .withColumn(\"PropertyCountInSuburb\",col(\"PropertyCountInSuburb\").cast(\"Int\"))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove bad landsize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 10 more fields]\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//remove where landsize is zero \n",
    "//enhance to be remove where landsize is less then X df_working = df_working.filter($\"Landsize\" < x)\n",
    "df_working = df_working.filter($\"Landsize\" =!= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Address on Street and Suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//split address on Street name and Suffix\n",
    "df_working = df_working.withColumn(\"StreetName\",split(col(\"Address\"),\" \").getItem(1)).\n",
    "    withColumn(\"StreetSuffix\",split(col(\"Address\"),\" \").getItem(2)).drop(\"Address\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n",
       "df_working: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// make first letter of suburb upper case\n",
    "df_working= df_working.withColumn(\"Suburb\", initcap(col(\"Suburb\")))\n",
    "\n",
    "//make type code upper\n",
    "df_working = df_working.withColumn(\"PropertyCountInSuburb\", initcap(col(\"PropertyCountInSuburb\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_not_null: org.apache.spark.sql.DataFrame = [Price: double, MethodOfSale: string ... 11 more fields]\n"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_not_null = df_working.na.drop()\n",
    "//debug df_not_null.count()\n",
    "// should = 17701 (with zero landsizes)\n",
    "// should = 15759 (with no zero landsizes)\n",
    "\n",
    "//debug df_not_null.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write down clean data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /tmp/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df_output: Unit = ()\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df_output = df_not_null.coalesce(1)\n",
    "   .write\n",
    "   .format(\"csv\")\n",
    "   .option(\"header\",\"true\")\n",
    "   .mode(\"overwrite\").option(\"sep\",\",\")\n",
    "   .save(\"hdfs://localhost:9000/tmp/output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the clean data to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "copyToLocal: `./../data-clean/cleanMelbourneData.csv': File exists\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -mkdir -p /tmp/output\n",
    "! hadoop fs -copyToLocal /tmp/output/\\*.csv ./../data-clean/cleanMelbourneData.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "Apache Spark (n.d.). _Spark Scala API (Scaladoc). Overview._ https://spark.apache.org/docs/latest/api/java/overview-summary.html\n",
    "\n",
    "Apache Spark (n.d.). _Basic Statistic._ https://spark.apache.org/docs/latest/ml-statistics.html\n",
    "\n",
    "Bahadoor N. (2020). _Spark Tutorials_ https://allaboutscala.com/big-data/spark/#dataframe-statistics-correlation\n",
    "\n",
    "Databricks. (2020). _Introduction to DataFrames - Scala._  https://docs.databricks.com/spark/latest/dataframes-datasets/introduction-to-dataframes-scala.html \n",
    "\n",
    "Grimaldi E. (2018). _Pandas vs. Spark: how to handle dataframes (Part II.)_  https://towardsdatascience.com/python-pandas-vs-scala-how-to-handle-dataframes-part-ii-d3e5efe8287d \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spylon-kernel",
   "language": "scala",
   "name": "spylon-kernel"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "help_links": [
    {
     "text": "MetaKernel Magics",
     "url": "https://metakernel.readthedocs.io/en/latest/source/README.html"
    }
   ],
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "0.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
